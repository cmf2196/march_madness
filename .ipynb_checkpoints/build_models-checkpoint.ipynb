{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_stats = pd.read_csv(\"clean_data/season_stats.csv\")\n",
    "teams = pd.read_csv(\"clean_data/teams.csv\")\n",
    "ncaa_games = pd.read_csv(\"clean_data/ncaa_games.csv\")\n",
    "schools = pd.read_csv(\"clean_data/schools.csv\")\n",
    "\n",
    "\n",
    "tournament_stats = pd.read_csv(\"tournament_data/prelim_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build our Feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new dataframe:\n",
    "data_matrix = ncaa_games.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feat_vec(r):\n",
    "    \"\"\" \n",
    "    This function builds the feature vec by subtracting team_1's stats from team_2's stats\n",
    "    \"\"\"\n",
    "    s1 = season_stats[season_stats[\"team_id\"] == r[\"team_1_id\"] ]\n",
    "    s2 = season_stats[season_stats[\"team_id\"] == r[\"team_2_id\"] ]\n",
    "    # reset the indices: this is important for subtraction\n",
    "    s2 = s2.reset_index(drop=True)\n",
    "    s1 = s1.reset_index(drop=True)   \n",
    "    # subtract the two stat sets\n",
    "    s3 = s1.sub(s2) \n",
    "    s3 = s3.iloc[0]\n",
    "    # append the difference to the game information\n",
    "    r = r.append(s3)\n",
    "    # somehow return the information.\n",
    "    return r\n",
    "data_matrix = data_matrix.apply(build_feat_vec , axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can get rid of the team_id columns because of the game_id. \n",
    "we will use the scores to create a binary: 0 if team 1 loses, 1 if team 1 wins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the result column\n",
    "data_matrix[\"result\"] = np.where(data_matrix['team_1_score'] > data_matrix['team_2_score'], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unwanted columns\n",
    "data_matrix = data_matrix.drop([\"team_1_id\" , \"team_2_id\" , \"team_id\" , \"team_1_score\" , \"team_2_score\"] , axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas: \n",
    "* Should we normalize these differences? \n",
    "* Should we normalize the teams before doing the differences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "* Do we want to create any new variable?\n",
    "* For now, lets simply reduce any overlap (i.e. FT = FTA * FT%)\n",
    "    + We do not need all three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = data_matrix.drop([\"game_id\" , \"FT\" , \"2P\" , \"3P\" , \"FG\" , \"TRB\"] , axis = 1)\n",
    "tournament_stats = tournament_stats.drop([ \"FT\" , \"2P\" , \"3P\" , \"FG\" , \"TRB\"] , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team</th>\n",
       "      <th>FGA</th>\n",
       "      <th>FG%</th>\n",
       "      <th>2PA</th>\n",
       "      <th>2P%</th>\n",
       "      <th>3PA</th>\n",
       "      <th>3P%</th>\n",
       "      <th>FTA</th>\n",
       "      <th>FT%</th>\n",
       "      <th>ORB</th>\n",
       "      <th>...</th>\n",
       "      <th>STL</th>\n",
       "      <th>BLK</th>\n",
       "      <th>TOV</th>\n",
       "      <th>PF</th>\n",
       "      <th>PTS/G</th>\n",
       "      <th>W-L%</th>\n",
       "      <th>SRS</th>\n",
       "      <th>SOS</th>\n",
       "      <th>PTS.1</th>\n",
       "      <th>Seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kansas</td>\n",
       "      <td>1758</td>\n",
       "      <td>0.484</td>\n",
       "      <td>1180</td>\n",
       "      <td>0.553</td>\n",
       "      <td>578</td>\n",
       "      <td>0.344</td>\n",
       "      <td>616</td>\n",
       "      <td>0.667</td>\n",
       "      <td>333</td>\n",
       "      <td>...</td>\n",
       "      <td>237</td>\n",
       "      <td>145</td>\n",
       "      <td>396</td>\n",
       "      <td>454</td>\n",
       "      <td>74.6</td>\n",
       "      <td>0.903</td>\n",
       "      <td>25.01</td>\n",
       "      <td>11.68</td>\n",
       "      <td>60.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>san-diego-state</td>\n",
       "      <td>1820</td>\n",
       "      <td>0.468</td>\n",
       "      <td>1055</td>\n",
       "      <td>0.532</td>\n",
       "      <td>765</td>\n",
       "      <td>0.380</td>\n",
       "      <td>519</td>\n",
       "      <td>0.771</td>\n",
       "      <td>297</td>\n",
       "      <td>...</td>\n",
       "      <td>221</td>\n",
       "      <td>88</td>\n",
       "      <td>344</td>\n",
       "      <td>534</td>\n",
       "      <td>74.8</td>\n",
       "      <td>0.938</td>\n",
       "      <td>18.35</td>\n",
       "      <td>3.90</td>\n",
       "      <td>59.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>maryland</td>\n",
       "      <td>1774</td>\n",
       "      <td>0.419</td>\n",
       "      <td>1009</td>\n",
       "      <td>0.501</td>\n",
       "      <td>765</td>\n",
       "      <td>0.311</td>\n",
       "      <td>669</td>\n",
       "      <td>0.747</td>\n",
       "      <td>348</td>\n",
       "      <td>...</td>\n",
       "      <td>142</td>\n",
       "      <td>134</td>\n",
       "      <td>350</td>\n",
       "      <td>463</td>\n",
       "      <td>71.8</td>\n",
       "      <td>0.774</td>\n",
       "      <td>17.82</td>\n",
       "      <td>10.49</td>\n",
       "      <td>64.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kentucky</td>\n",
       "      <td>1723</td>\n",
       "      <td>0.461</td>\n",
       "      <td>1265</td>\n",
       "      <td>0.504</td>\n",
       "      <td>458</td>\n",
       "      <td>0.343</td>\n",
       "      <td>703</td>\n",
       "      <td>0.797</td>\n",
       "      <td>300</td>\n",
       "      <td>...</td>\n",
       "      <td>162</td>\n",
       "      <td>153</td>\n",
       "      <td>387</td>\n",
       "      <td>568</td>\n",
       "      <td>74.4</td>\n",
       "      <td>0.806</td>\n",
       "      <td>15.72</td>\n",
       "      <td>7.40</td>\n",
       "      <td>66.1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>butler</td>\n",
       "      <td>1687</td>\n",
       "      <td>0.460</td>\n",
       "      <td>1066</td>\n",
       "      <td>0.529</td>\n",
       "      <td>621</td>\n",
       "      <td>0.341</td>\n",
       "      <td>496</td>\n",
       "      <td>0.728</td>\n",
       "      <td>280</td>\n",
       "      <td>...</td>\n",
       "      <td>165</td>\n",
       "      <td>82</td>\n",
       "      <td>349</td>\n",
       "      <td>519</td>\n",
       "      <td>68.5</td>\n",
       "      <td>0.710</td>\n",
       "      <td>15.59</td>\n",
       "      <td>9.17</td>\n",
       "      <td>62.1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>auburn</td>\n",
       "      <td>1868</td>\n",
       "      <td>0.433</td>\n",
       "      <td>1044</td>\n",
       "      <td>0.533</td>\n",
       "      <td>824</td>\n",
       "      <td>0.306</td>\n",
       "      <td>818</td>\n",
       "      <td>0.674</td>\n",
       "      <td>421</td>\n",
       "      <td>...</td>\n",
       "      <td>199</td>\n",
       "      <td>151</td>\n",
       "      <td>399</td>\n",
       "      <td>596</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.806</td>\n",
       "      <td>15.01</td>\n",
       "      <td>7.36</td>\n",
       "      <td>70.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>west-virginia</td>\n",
       "      <td>1833</td>\n",
       "      <td>0.422</td>\n",
       "      <td>1291</td>\n",
       "      <td>0.479</td>\n",
       "      <td>542</td>\n",
       "      <td>0.286</td>\n",
       "      <td>742</td>\n",
       "      <td>0.642</td>\n",
       "      <td>470</td>\n",
       "      <td>...</td>\n",
       "      <td>238</td>\n",
       "      <td>111</td>\n",
       "      <td>447</td>\n",
       "      <td>572</td>\n",
       "      <td>70.3</td>\n",
       "      <td>0.677</td>\n",
       "      <td>18.49</td>\n",
       "      <td>10.62</td>\n",
       "      <td>62.4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>saint-marys-ca</td>\n",
       "      <td>1879</td>\n",
       "      <td>0.480</td>\n",
       "      <td>1199</td>\n",
       "      <td>0.529</td>\n",
       "      <td>680</td>\n",
       "      <td>0.393</td>\n",
       "      <td>564</td>\n",
       "      <td>0.752</td>\n",
       "      <td>257</td>\n",
       "      <td>...</td>\n",
       "      <td>224</td>\n",
       "      <td>66</td>\n",
       "      <td>325</td>\n",
       "      <td>544</td>\n",
       "      <td>75.5</td>\n",
       "      <td>0.788</td>\n",
       "      <td>12.48</td>\n",
       "      <td>4.67</td>\n",
       "      <td>66.4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>oklahoma</td>\n",
       "      <td>1808</td>\n",
       "      <td>0.420</td>\n",
       "      <td>1104</td>\n",
       "      <td>0.486</td>\n",
       "      <td>704</td>\n",
       "      <td>0.318</td>\n",
       "      <td>565</td>\n",
       "      <td>0.766</td>\n",
       "      <td>259</td>\n",
       "      <td>...</td>\n",
       "      <td>190</td>\n",
       "      <td>135</td>\n",
       "      <td>345</td>\n",
       "      <td>415</td>\n",
       "      <td>70.2</td>\n",
       "      <td>0.613</td>\n",
       "      <td>13.50</td>\n",
       "      <td>10.67</td>\n",
       "      <td>67.4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>texas-tech</td>\n",
       "      <td>1724</td>\n",
       "      <td>0.452</td>\n",
       "      <td>1093</td>\n",
       "      <td>0.508</td>\n",
       "      <td>631</td>\n",
       "      <td>0.355</td>\n",
       "      <td>594</td>\n",
       "      <td>0.754</td>\n",
       "      <td>279</td>\n",
       "      <td>...</td>\n",
       "      <td>239</td>\n",
       "      <td>124</td>\n",
       "      <td>423</td>\n",
       "      <td>545</td>\n",
       "      <td>71.9</td>\n",
       "      <td>0.581</td>\n",
       "      <td>16.81</td>\n",
       "      <td>8.39</td>\n",
       "      <td>63.5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>arizona-state</td>\n",
       "      <td>1904</td>\n",
       "      <td>0.429</td>\n",
       "      <td>1201</td>\n",
       "      <td>0.490</td>\n",
       "      <td>703</td>\n",
       "      <td>0.323</td>\n",
       "      <td>617</td>\n",
       "      <td>0.695</td>\n",
       "      <td>312</td>\n",
       "      <td>...</td>\n",
       "      <td>249</td>\n",
       "      <td>104</td>\n",
       "      <td>408</td>\n",
       "      <td>577</td>\n",
       "      <td>73.8</td>\n",
       "      <td>0.645</td>\n",
       "      <td>11.41</td>\n",
       "      <td>8.06</td>\n",
       "      <td>70.5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>purdue</td>\n",
       "      <td>1867</td>\n",
       "      <td>0.418</td>\n",
       "      <td>1192</td>\n",
       "      <td>0.467</td>\n",
       "      <td>675</td>\n",
       "      <td>0.330</td>\n",
       "      <td>482</td>\n",
       "      <td>0.687</td>\n",
       "      <td>394</td>\n",
       "      <td>...</td>\n",
       "      <td>202</td>\n",
       "      <td>109</td>\n",
       "      <td>357</td>\n",
       "      <td>544</td>\n",
       "      <td>68.2</td>\n",
       "      <td>0.516</td>\n",
       "      <td>16.30</td>\n",
       "      <td>10.75</td>\n",
       "      <td>62.6</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>richmond</td>\n",
       "      <td>1817</td>\n",
       "      <td>0.466</td>\n",
       "      <td>1129</td>\n",
       "      <td>0.530</td>\n",
       "      <td>688</td>\n",
       "      <td>0.362</td>\n",
       "      <td>505</td>\n",
       "      <td>0.776</td>\n",
       "      <td>234</td>\n",
       "      <td>...</td>\n",
       "      <td>252</td>\n",
       "      <td>78</td>\n",
       "      <td>338</td>\n",
       "      <td>479</td>\n",
       "      <td>75.3</td>\n",
       "      <td>0.774</td>\n",
       "      <td>11.40</td>\n",
       "      <td>2.85</td>\n",
       "      <td>66.8</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tennessee</td>\n",
       "      <td>1656</td>\n",
       "      <td>0.432</td>\n",
       "      <td>1058</td>\n",
       "      <td>0.499</td>\n",
       "      <td>598</td>\n",
       "      <td>0.313</td>\n",
       "      <td>617</td>\n",
       "      <td>0.749</td>\n",
       "      <td>311</td>\n",
       "      <td>...</td>\n",
       "      <td>186</td>\n",
       "      <td>174</td>\n",
       "      <td>423</td>\n",
       "      <td>563</td>\n",
       "      <td>67.1</td>\n",
       "      <td>0.548</td>\n",
       "      <td>11.18</td>\n",
       "      <td>8.31</td>\n",
       "      <td>64.2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rhode-island</td>\n",
       "      <td>1851</td>\n",
       "      <td>0.423</td>\n",
       "      <td>1312</td>\n",
       "      <td>0.467</td>\n",
       "      <td>539</td>\n",
       "      <td>0.315</td>\n",
       "      <td>686</td>\n",
       "      <td>0.676</td>\n",
       "      <td>359</td>\n",
       "      <td>...</td>\n",
       "      <td>252</td>\n",
       "      <td>116</td>\n",
       "      <td>355</td>\n",
       "      <td>519</td>\n",
       "      <td>73.3</td>\n",
       "      <td>0.700</td>\n",
       "      <td>9.25</td>\n",
       "      <td>4.78</td>\n",
       "      <td>68.9</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mississippi-state</td>\n",
       "      <td>1713</td>\n",
       "      <td>0.468</td>\n",
       "      <td>1234</td>\n",
       "      <td>0.519</td>\n",
       "      <td>479</td>\n",
       "      <td>0.336</td>\n",
       "      <td>676</td>\n",
       "      <td>0.754</td>\n",
       "      <td>371</td>\n",
       "      <td>...</td>\n",
       "      <td>174</td>\n",
       "      <td>158</td>\n",
       "      <td>431</td>\n",
       "      <td>542</td>\n",
       "      <td>73.4</td>\n",
       "      <td>0.645</td>\n",
       "      <td>12.20</td>\n",
       "      <td>6.95</td>\n",
       "      <td>68.1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gonzaga</td>\n",
       "      <td>1977</td>\n",
       "      <td>0.514</td>\n",
       "      <td>1355</td>\n",
       "      <td>0.573</td>\n",
       "      <td>622</td>\n",
       "      <td>0.387</td>\n",
       "      <td>765</td>\n",
       "      <td>0.688</td>\n",
       "      <td>362</td>\n",
       "      <td>...</td>\n",
       "      <td>238</td>\n",
       "      <td>123</td>\n",
       "      <td>362</td>\n",
       "      <td>467</td>\n",
       "      <td>87.5</td>\n",
       "      <td>0.938</td>\n",
       "      <td>22.50</td>\n",
       "      <td>2.88</td>\n",
       "      <td>67.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>florida-state</td>\n",
       "      <td>1848</td>\n",
       "      <td>0.457</td>\n",
       "      <td>1193</td>\n",
       "      <td>0.515</td>\n",
       "      <td>655</td>\n",
       "      <td>0.351</td>\n",
       "      <td>557</td>\n",
       "      <td>0.750</td>\n",
       "      <td>350</td>\n",
       "      <td>...</td>\n",
       "      <td>273</td>\n",
       "      <td>162</td>\n",
       "      <td>410</td>\n",
       "      <td>542</td>\n",
       "      <td>75.4</td>\n",
       "      <td>0.839</td>\n",
       "      <td>17.27</td>\n",
       "      <td>7.63</td>\n",
       "      <td>65.7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>louisville</td>\n",
       "      <td>1787</td>\n",
       "      <td>0.453</td>\n",
       "      <td>1108</td>\n",
       "      <td>0.501</td>\n",
       "      <td>679</td>\n",
       "      <td>0.376</td>\n",
       "      <td>576</td>\n",
       "      <td>0.731</td>\n",
       "      <td>335</td>\n",
       "      <td>...</td>\n",
       "      <td>157</td>\n",
       "      <td>83</td>\n",
       "      <td>376</td>\n",
       "      <td>524</td>\n",
       "      <td>74.1</td>\n",
       "      <td>0.774</td>\n",
       "      <td>17.93</td>\n",
       "      <td>7.55</td>\n",
       "      <td>63.7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>villanova</td>\n",
       "      <td>1806</td>\n",
       "      <td>0.441</td>\n",
       "      <td>952</td>\n",
       "      <td>0.515</td>\n",
       "      <td>854</td>\n",
       "      <td>0.359</td>\n",
       "      <td>485</td>\n",
       "      <td>0.759</td>\n",
       "      <td>271</td>\n",
       "      <td>...</td>\n",
       "      <td>175</td>\n",
       "      <td>81</td>\n",
       "      <td>335</td>\n",
       "      <td>471</td>\n",
       "      <td>73.2</td>\n",
       "      <td>0.774</td>\n",
       "      <td>16.84</td>\n",
       "      <td>10.03</td>\n",
       "      <td>66.4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>penn-state</td>\n",
       "      <td>1931</td>\n",
       "      <td>0.431</td>\n",
       "      <td>1166</td>\n",
       "      <td>0.500</td>\n",
       "      <td>765</td>\n",
       "      <td>0.325</td>\n",
       "      <td>598</td>\n",
       "      <td>0.692</td>\n",
       "      <td>348</td>\n",
       "      <td>...</td>\n",
       "      <td>233</td>\n",
       "      <td>150</td>\n",
       "      <td>348</td>\n",
       "      <td>543</td>\n",
       "      <td>75.1</td>\n",
       "      <td>0.677</td>\n",
       "      <td>16.12</td>\n",
       "      <td>9.25</td>\n",
       "      <td>68.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>iowa</td>\n",
       "      <td>1868</td>\n",
       "      <td>0.451</td>\n",
       "      <td>1165</td>\n",
       "      <td>0.513</td>\n",
       "      <td>703</td>\n",
       "      <td>0.347</td>\n",
       "      <td>643</td>\n",
       "      <td>0.750</td>\n",
       "      <td>356</td>\n",
       "      <td>...</td>\n",
       "      <td>191</td>\n",
       "      <td>120</td>\n",
       "      <td>375</td>\n",
       "      <td>496</td>\n",
       "      <td>77.7</td>\n",
       "      <td>0.645</td>\n",
       "      <td>16.05</td>\n",
       "      <td>10.59</td>\n",
       "      <td>72.3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>arizona</td>\n",
       "      <td>1847</td>\n",
       "      <td>0.448</td>\n",
       "      <td>1210</td>\n",
       "      <td>0.498</td>\n",
       "      <td>637</td>\n",
       "      <td>0.352</td>\n",
       "      <td>670</td>\n",
       "      <td>0.731</td>\n",
       "      <td>338</td>\n",
       "      <td>...</td>\n",
       "      <td>207</td>\n",
       "      <td>115</td>\n",
       "      <td>365</td>\n",
       "      <td>572</td>\n",
       "      <td>76.4</td>\n",
       "      <td>0.645</td>\n",
       "      <td>19.46</td>\n",
       "      <td>8.24</td>\n",
       "      <td>65.2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>houston</td>\n",
       "      <td>1868</td>\n",
       "      <td>0.422</td>\n",
       "      <td>1227</td>\n",
       "      <td>0.466</td>\n",
       "      <td>641</td>\n",
       "      <td>0.337</td>\n",
       "      <td>615</td>\n",
       "      <td>0.730</td>\n",
       "      <td>447</td>\n",
       "      <td>...</td>\n",
       "      <td>161</td>\n",
       "      <td>151</td>\n",
       "      <td>369</td>\n",
       "      <td>608</td>\n",
       "      <td>72.3</td>\n",
       "      <td>0.742</td>\n",
       "      <td>16.47</td>\n",
       "      <td>6.31</td>\n",
       "      <td>62.1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>louisiana-state</td>\n",
       "      <td>1880</td>\n",
       "      <td>0.471</td>\n",
       "      <td>1235</td>\n",
       "      <td>0.547</td>\n",
       "      <td>645</td>\n",
       "      <td>0.327</td>\n",
       "      <td>666</td>\n",
       "      <td>0.769</td>\n",
       "      <td>375</td>\n",
       "      <td>...</td>\n",
       "      <td>195</td>\n",
       "      <td>121</td>\n",
       "      <td>390</td>\n",
       "      <td>495</td>\n",
       "      <td>80.5</td>\n",
       "      <td>0.677</td>\n",
       "      <td>14.41</td>\n",
       "      <td>7.18</td>\n",
       "      <td>73.3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>xavier</td>\n",
       "      <td>1852</td>\n",
       "      <td>0.435</td>\n",
       "      <td>1205</td>\n",
       "      <td>0.500</td>\n",
       "      <td>647</td>\n",
       "      <td>0.315</td>\n",
       "      <td>576</td>\n",
       "      <td>0.656</td>\n",
       "      <td>371</td>\n",
       "      <td>...</td>\n",
       "      <td>191</td>\n",
       "      <td>113</td>\n",
       "      <td>435</td>\n",
       "      <td>521</td>\n",
       "      <td>70.8</td>\n",
       "      <td>0.613</td>\n",
       "      <td>12.85</td>\n",
       "      <td>9.53</td>\n",
       "      <td>67.5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>texas</td>\n",
       "      <td>1730</td>\n",
       "      <td>0.428</td>\n",
       "      <td>988</td>\n",
       "      <td>0.498</td>\n",
       "      <td>742</td>\n",
       "      <td>0.334</td>\n",
       "      <td>381</td>\n",
       "      <td>0.685</td>\n",
       "      <td>278</td>\n",
       "      <td>...</td>\n",
       "      <td>171</td>\n",
       "      <td>134</td>\n",
       "      <td>420</td>\n",
       "      <td>568</td>\n",
       "      <td>64.2</td>\n",
       "      <td>0.613</td>\n",
       "      <td>10.46</td>\n",
       "      <td>9.59</td>\n",
       "      <td>63.3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>tulsa</td>\n",
       "      <td>1652</td>\n",
       "      <td>0.441</td>\n",
       "      <td>1086</td>\n",
       "      <td>0.503</td>\n",
       "      <td>566</td>\n",
       "      <td>0.322</td>\n",
       "      <td>657</td>\n",
       "      <td>0.708</td>\n",
       "      <td>268</td>\n",
       "      <td>...</td>\n",
       "      <td>234</td>\n",
       "      <td>70</td>\n",
       "      <td>417</td>\n",
       "      <td>469</td>\n",
       "      <td>67.8</td>\n",
       "      <td>0.677</td>\n",
       "      <td>7.93</td>\n",
       "      <td>3.60</td>\n",
       "      <td>63.5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>vermont</td>\n",
       "      <td>1819</td>\n",
       "      <td>0.446</td>\n",
       "      <td>1031</td>\n",
       "      <td>0.535</td>\n",
       "      <td>788</td>\n",
       "      <td>0.330</td>\n",
       "      <td>555</td>\n",
       "      <td>0.714</td>\n",
       "      <td>308</td>\n",
       "      <td>...</td>\n",
       "      <td>189</td>\n",
       "      <td>109</td>\n",
       "      <td>363</td>\n",
       "      <td>518</td>\n",
       "      <td>71.3</td>\n",
       "      <td>0.781</td>\n",
       "      <td>6.28</td>\n",
       "      <td>-4.76</td>\n",
       "      <td>59.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>belmont</td>\n",
       "      <td>2074</td>\n",
       "      <td>0.475</td>\n",
       "      <td>1166</td>\n",
       "      <td>0.571</td>\n",
       "      <td>908</td>\n",
       "      <td>0.352</td>\n",
       "      <td>470</td>\n",
       "      <td>0.713</td>\n",
       "      <td>290</td>\n",
       "      <td>...</td>\n",
       "      <td>249</td>\n",
       "      <td>106</td>\n",
       "      <td>392</td>\n",
       "      <td>497</td>\n",
       "      <td>79.6</td>\n",
       "      <td>0.788</td>\n",
       "      <td>4.31</td>\n",
       "      <td>-6.22</td>\n",
       "      <td>67.9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>duke</td>\n",
       "      <td>1957</td>\n",
       "      <td>0.470</td>\n",
       "      <td>1340</td>\n",
       "      <td>0.525</td>\n",
       "      <td>617</td>\n",
       "      <td>0.352</td>\n",
       "      <td>696</td>\n",
       "      <td>0.720</td>\n",
       "      <td>394</td>\n",
       "      <td>...</td>\n",
       "      <td>258</td>\n",
       "      <td>188</td>\n",
       "      <td>409</td>\n",
       "      <td>557</td>\n",
       "      <td>82.5</td>\n",
       "      <td>0.806</td>\n",
       "      <td>22.60</td>\n",
       "      <td>8.06</td>\n",
       "      <td>68.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>oregon</td>\n",
       "      <td>1825</td>\n",
       "      <td>0.470</td>\n",
       "      <td>1145</td>\n",
       "      <td>0.514</td>\n",
       "      <td>680</td>\n",
       "      <td>0.396</td>\n",
       "      <td>527</td>\n",
       "      <td>0.700</td>\n",
       "      <td>336</td>\n",
       "      <td>...</td>\n",
       "      <td>241</td>\n",
       "      <td>102</td>\n",
       "      <td>368</td>\n",
       "      <td>538</td>\n",
       "      <td>75.9</td>\n",
       "      <td>0.774</td>\n",
       "      <td>18.23</td>\n",
       "      <td>9.10</td>\n",
       "      <td>66.7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>brigham-young</td>\n",
       "      <td>1897</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1112</td>\n",
       "      <td>0.555</td>\n",
       "      <td>785</td>\n",
       "      <td>0.423</td>\n",
       "      <td>454</td>\n",
       "      <td>0.700</td>\n",
       "      <td>209</td>\n",
       "      <td>...</td>\n",
       "      <td>186</td>\n",
       "      <td>67</td>\n",
       "      <td>353</td>\n",
       "      <td>513</td>\n",
       "      <td>79.6</td>\n",
       "      <td>0.750</td>\n",
       "      <td>16.97</td>\n",
       "      <td>6.49</td>\n",
       "      <td>68.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>colorado</td>\n",
       "      <td>1721</td>\n",
       "      <td>0.439</td>\n",
       "      <td>1055</td>\n",
       "      <td>0.492</td>\n",
       "      <td>666</td>\n",
       "      <td>0.356</td>\n",
       "      <td>611</td>\n",
       "      <td>0.738</td>\n",
       "      <td>312</td>\n",
       "      <td>...</td>\n",
       "      <td>179</td>\n",
       "      <td>94</td>\n",
       "      <td>411</td>\n",
       "      <td>451</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.677</td>\n",
       "      <td>15.64</td>\n",
       "      <td>8.38</td>\n",
       "      <td>63.7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>illinois</td>\n",
       "      <td>1831</td>\n",
       "      <td>0.443</td>\n",
       "      <td>1265</td>\n",
       "      <td>0.503</td>\n",
       "      <td>566</td>\n",
       "      <td>0.309</td>\n",
       "      <td>607</td>\n",
       "      <td>0.728</td>\n",
       "      <td>393</td>\n",
       "      <td>...</td>\n",
       "      <td>151</td>\n",
       "      <td>89</td>\n",
       "      <td>375</td>\n",
       "      <td>497</td>\n",
       "      <td>72.2</td>\n",
       "      <td>0.677</td>\n",
       "      <td>15.30</td>\n",
       "      <td>9.86</td>\n",
       "      <td>65.3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>indiana</td>\n",
       "      <td>1759</td>\n",
       "      <td>0.442</td>\n",
       "      <td>1241</td>\n",
       "      <td>0.492</td>\n",
       "      <td>518</td>\n",
       "      <td>0.322</td>\n",
       "      <td>699</td>\n",
       "      <td>0.681</td>\n",
       "      <td>353</td>\n",
       "      <td>...</td>\n",
       "      <td>175</td>\n",
       "      <td>128</td>\n",
       "      <td>393</td>\n",
       "      <td>532</td>\n",
       "      <td>70.9</td>\n",
       "      <td>0.613</td>\n",
       "      <td>14.67</td>\n",
       "      <td>10.61</td>\n",
       "      <td>66.8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>providence</td>\n",
       "      <td>1852</td>\n",
       "      <td>0.413</td>\n",
       "      <td>1174</td>\n",
       "      <td>0.459</td>\n",
       "      <td>678</td>\n",
       "      <td>0.332</td>\n",
       "      <td>664</td>\n",
       "      <td>0.694</td>\n",
       "      <td>406</td>\n",
       "      <td>...</td>\n",
       "      <td>257</td>\n",
       "      <td>102</td>\n",
       "      <td>417</td>\n",
       "      <td>581</td>\n",
       "      <td>71.4</td>\n",
       "      <td>0.613</td>\n",
       "      <td>13.47</td>\n",
       "      <td>8.21</td>\n",
       "      <td>66.2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>florida</td>\n",
       "      <td>1729</td>\n",
       "      <td>0.457</td>\n",
       "      <td>1093</td>\n",
       "      <td>0.522</td>\n",
       "      <td>636</td>\n",
       "      <td>0.347</td>\n",
       "      <td>588</td>\n",
       "      <td>0.723</td>\n",
       "      <td>313</td>\n",
       "      <td>...</td>\n",
       "      <td>178</td>\n",
       "      <td>121</td>\n",
       "      <td>369</td>\n",
       "      <td>528</td>\n",
       "      <td>71.9</td>\n",
       "      <td>0.613</td>\n",
       "      <td>14.94</td>\n",
       "      <td>9.10</td>\n",
       "      <td>66.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>northern-iowa</td>\n",
       "      <td>1776</td>\n",
       "      <td>0.475</td>\n",
       "      <td>1039</td>\n",
       "      <td>0.536</td>\n",
       "      <td>737</td>\n",
       "      <td>0.389</td>\n",
       "      <td>468</td>\n",
       "      <td>0.759</td>\n",
       "      <td>294</td>\n",
       "      <td>...</td>\n",
       "      <td>143</td>\n",
       "      <td>71</td>\n",
       "      <td>372</td>\n",
       "      <td>443</td>\n",
       "      <td>75.2</td>\n",
       "      <td>0.806</td>\n",
       "      <td>9.86</td>\n",
       "      <td>1.49</td>\n",
       "      <td>64.7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>liberty</td>\n",
       "      <td>1754</td>\n",
       "      <td>0.479</td>\n",
       "      <td>1044</td>\n",
       "      <td>0.557</td>\n",
       "      <td>710</td>\n",
       "      <td>0.366</td>\n",
       "      <td>548</td>\n",
       "      <td>0.712</td>\n",
       "      <td>250</td>\n",
       "      <td>...</td>\n",
       "      <td>192</td>\n",
       "      <td>96</td>\n",
       "      <td>318</td>\n",
       "      <td>456</td>\n",
       "      <td>68.6</td>\n",
       "      <td>0.882</td>\n",
       "      <td>5.17</td>\n",
       "      <td>-7.24</td>\n",
       "      <td>53.8</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>akron</td>\n",
       "      <td>1789</td>\n",
       "      <td>0.440</td>\n",
       "      <td>994</td>\n",
       "      <td>0.504</td>\n",
       "      <td>795</td>\n",
       "      <td>0.361</td>\n",
       "      <td>629</td>\n",
       "      <td>0.771</td>\n",
       "      <td>302</td>\n",
       "      <td>...</td>\n",
       "      <td>158</td>\n",
       "      <td>91</td>\n",
       "      <td>397</td>\n",
       "      <td>548</td>\n",
       "      <td>75.7</td>\n",
       "      <td>0.774</td>\n",
       "      <td>7.11</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>66.5</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>yale</td>\n",
       "      <td>1726</td>\n",
       "      <td>0.469</td>\n",
       "      <td>1012</td>\n",
       "      <td>0.538</td>\n",
       "      <td>714</td>\n",
       "      <td>0.371</td>\n",
       "      <td>570</td>\n",
       "      <td>0.712</td>\n",
       "      <td>273</td>\n",
       "      <td>...</td>\n",
       "      <td>188</td>\n",
       "      <td>101</td>\n",
       "      <td>389</td>\n",
       "      <td>449</td>\n",
       "      <td>76.3</td>\n",
       "      <td>0.767</td>\n",
       "      <td>6.62</td>\n",
       "      <td>-1.53</td>\n",
       "      <td>65.1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>radford</td>\n",
       "      <td>1813</td>\n",
       "      <td>0.464</td>\n",
       "      <td>1106</td>\n",
       "      <td>0.533</td>\n",
       "      <td>707</td>\n",
       "      <td>0.355</td>\n",
       "      <td>575</td>\n",
       "      <td>0.642</td>\n",
       "      <td>340</td>\n",
       "      <td>...</td>\n",
       "      <td>170</td>\n",
       "      <td>68</td>\n",
       "      <td>342</td>\n",
       "      <td>561</td>\n",
       "      <td>71.9</td>\n",
       "      <td>0.656</td>\n",
       "      <td>-2.66</td>\n",
       "      <td>-5.70</td>\n",
       "      <td>66.1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>hofstra</td>\n",
       "      <td>1934</td>\n",
       "      <td>0.451</td>\n",
       "      <td>1206</td>\n",
       "      <td>0.498</td>\n",
       "      <td>728</td>\n",
       "      <td>0.374</td>\n",
       "      <td>652</td>\n",
       "      <td>0.779</td>\n",
       "      <td>339</td>\n",
       "      <td>...</td>\n",
       "      <td>231</td>\n",
       "      <td>70</td>\n",
       "      <td>374</td>\n",
       "      <td>475</td>\n",
       "      <td>76.5</td>\n",
       "      <td>0.758</td>\n",
       "      <td>2.88</td>\n",
       "      <td>-3.97</td>\n",
       "      <td>68.6</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>dayton</td>\n",
       "      <td>1725</td>\n",
       "      <td>0.525</td>\n",
       "      <td>1053</td>\n",
       "      <td>0.623</td>\n",
       "      <td>672</td>\n",
       "      <td>0.371</td>\n",
       "      <td>585</td>\n",
       "      <td>0.720</td>\n",
       "      <td>242</td>\n",
       "      <td>...</td>\n",
       "      <td>176</td>\n",
       "      <td>109</td>\n",
       "      <td>377</td>\n",
       "      <td>529</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.935</td>\n",
       "      <td>19.22</td>\n",
       "      <td>3.74</td>\n",
       "      <td>64.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>michigan-state</td>\n",
       "      <td>1833</td>\n",
       "      <td>0.461</td>\n",
       "      <td>1146</td>\n",
       "      <td>0.529</td>\n",
       "      <td>687</td>\n",
       "      <td>0.348</td>\n",
       "      <td>565</td>\n",
       "      <td>0.750</td>\n",
       "      <td>344</td>\n",
       "      <td>...</td>\n",
       "      <td>159</td>\n",
       "      <td>156</td>\n",
       "      <td>389</td>\n",
       "      <td>524</td>\n",
       "      <td>75.9</td>\n",
       "      <td>0.710</td>\n",
       "      <td>21.92</td>\n",
       "      <td>10.76</td>\n",
       "      <td>64.7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>creighton</td>\n",
       "      <td>1839</td>\n",
       "      <td>0.471</td>\n",
       "      <td>1056</td>\n",
       "      <td>0.534</td>\n",
       "      <td>783</td>\n",
       "      <td>0.386</td>\n",
       "      <td>533</td>\n",
       "      <td>0.739</td>\n",
       "      <td>246</td>\n",
       "      <td>...</td>\n",
       "      <td>203</td>\n",
       "      <td>79</td>\n",
       "      <td>337</td>\n",
       "      <td>472</td>\n",
       "      <td>78.3</td>\n",
       "      <td>0.774</td>\n",
       "      <td>17.25</td>\n",
       "      <td>9.61</td>\n",
       "      <td>69.7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>ohio-state</td>\n",
       "      <td>1690</td>\n",
       "      <td>0.446</td>\n",
       "      <td>990</td>\n",
       "      <td>0.497</td>\n",
       "      <td>700</td>\n",
       "      <td>0.373</td>\n",
       "      <td>621</td>\n",
       "      <td>0.752</td>\n",
       "      <td>308</td>\n",
       "      <td>...</td>\n",
       "      <td>156</td>\n",
       "      <td>107</td>\n",
       "      <td>394</td>\n",
       "      <td>531</td>\n",
       "      <td>72.1</td>\n",
       "      <td>0.677</td>\n",
       "      <td>19.37</td>\n",
       "      <td>10.24</td>\n",
       "      <td>62.9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>wisconsin</td>\n",
       "      <td>1718</td>\n",
       "      <td>0.425</td>\n",
       "      <td>954</td>\n",
       "      <td>0.483</td>\n",
       "      <td>764</td>\n",
       "      <td>0.352</td>\n",
       "      <td>462</td>\n",
       "      <td>0.764</td>\n",
       "      <td>251</td>\n",
       "      <td>...</td>\n",
       "      <td>138</td>\n",
       "      <td>114</td>\n",
       "      <td>309</td>\n",
       "      <td>476</td>\n",
       "      <td>67.2</td>\n",
       "      <td>0.677</td>\n",
       "      <td>15.96</td>\n",
       "      <td>10.96</td>\n",
       "      <td>62.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>michigan</td>\n",
       "      <td>1864</td>\n",
       "      <td>0.462</td>\n",
       "      <td>1135</td>\n",
       "      <td>0.541</td>\n",
       "      <td>729</td>\n",
       "      <td>0.339</td>\n",
       "      <td>498</td>\n",
       "      <td>0.717</td>\n",
       "      <td>267</td>\n",
       "      <td>...</td>\n",
       "      <td>169</td>\n",
       "      <td>140</td>\n",
       "      <td>332</td>\n",
       "      <td>470</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.613</td>\n",
       "      <td>17.96</td>\n",
       "      <td>11.19</td>\n",
       "      <td>68.3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>virginia</td>\n",
       "      <td>1515</td>\n",
       "      <td>0.413</td>\n",
       "      <td>955</td>\n",
       "      <td>0.477</td>\n",
       "      <td>560</td>\n",
       "      <td>0.304</td>\n",
       "      <td>403</td>\n",
       "      <td>0.717</td>\n",
       "      <td>243</td>\n",
       "      <td>...</td>\n",
       "      <td>161</td>\n",
       "      <td>147</td>\n",
       "      <td>368</td>\n",
       "      <td>397</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.767</td>\n",
       "      <td>12.19</td>\n",
       "      <td>7.53</td>\n",
       "      <td>52.4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>marquette</td>\n",
       "      <td>1764</td>\n",
       "      <td>0.430</td>\n",
       "      <td>977</td>\n",
       "      <td>0.469</td>\n",
       "      <td>787</td>\n",
       "      <td>0.382</td>\n",
       "      <td>692</td>\n",
       "      <td>0.743</td>\n",
       "      <td>317</td>\n",
       "      <td>...</td>\n",
       "      <td>154</td>\n",
       "      <td>127</td>\n",
       "      <td>410</td>\n",
       "      <td>572</td>\n",
       "      <td>77.8</td>\n",
       "      <td>0.600</td>\n",
       "      <td>15.41</td>\n",
       "      <td>9.27</td>\n",
       "      <td>71.6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>southern-california</td>\n",
       "      <td>1780</td>\n",
       "      <td>0.439</td>\n",
       "      <td>1209</td>\n",
       "      <td>0.481</td>\n",
       "      <td>571</td>\n",
       "      <td>0.350</td>\n",
       "      <td>630</td>\n",
       "      <td>0.662</td>\n",
       "      <td>333</td>\n",
       "      <td>...</td>\n",
       "      <td>207</td>\n",
       "      <td>152</td>\n",
       "      <td>448</td>\n",
       "      <td>519</td>\n",
       "      <td>70.3</td>\n",
       "      <td>0.710</td>\n",
       "      <td>11.52</td>\n",
       "      <td>7.23</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>rutgers</td>\n",
       "      <td>1847</td>\n",
       "      <td>0.447</td>\n",
       "      <td>1293</td>\n",
       "      <td>0.505</td>\n",
       "      <td>554</td>\n",
       "      <td>0.310</td>\n",
       "      <td>537</td>\n",
       "      <td>0.644</td>\n",
       "      <td>365</td>\n",
       "      <td>...</td>\n",
       "      <td>203</td>\n",
       "      <td>142</td>\n",
       "      <td>379</td>\n",
       "      <td>520</td>\n",
       "      <td>69.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>15.73</td>\n",
       "      <td>9.69</td>\n",
       "      <td>62.6</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>east-tennessee-state</td>\n",
       "      <td>1936</td>\n",
       "      <td>0.475</td>\n",
       "      <td>1184</td>\n",
       "      <td>0.557</td>\n",
       "      <td>752</td>\n",
       "      <td>0.346</td>\n",
       "      <td>713</td>\n",
       "      <td>0.694</td>\n",
       "      <td>370</td>\n",
       "      <td>...</td>\n",
       "      <td>278</td>\n",
       "      <td>121</td>\n",
       "      <td>412</td>\n",
       "      <td>598</td>\n",
       "      <td>76.3</td>\n",
       "      <td>0.882</td>\n",
       "      <td>7.63</td>\n",
       "      <td>-1.82</td>\n",
       "      <td>63.1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>stephen-f-austin</td>\n",
       "      <td>1806</td>\n",
       "      <td>0.491</td>\n",
       "      <td>1312</td>\n",
       "      <td>0.540</td>\n",
       "      <td>494</td>\n",
       "      <td>0.362</td>\n",
       "      <td>790</td>\n",
       "      <td>0.691</td>\n",
       "      <td>406</td>\n",
       "      <td>...</td>\n",
       "      <td>324</td>\n",
       "      <td>101</td>\n",
       "      <td>550</td>\n",
       "      <td>649</td>\n",
       "      <td>80.6</td>\n",
       "      <td>0.903</td>\n",
       "      <td>4.00</td>\n",
       "      <td>-6.93</td>\n",
       "      <td>67.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>cincinnati</td>\n",
       "      <td>1713</td>\n",
       "      <td>0.445</td>\n",
       "      <td>1095</td>\n",
       "      <td>0.517</td>\n",
       "      <td>618</td>\n",
       "      <td>0.319</td>\n",
       "      <td>678</td>\n",
       "      <td>0.677</td>\n",
       "      <td>329</td>\n",
       "      <td>...</td>\n",
       "      <td>196</td>\n",
       "      <td>146</td>\n",
       "      <td>432</td>\n",
       "      <td>540</td>\n",
       "      <td>72.7</td>\n",
       "      <td>0.667</td>\n",
       "      <td>12.37</td>\n",
       "      <td>6.77</td>\n",
       "      <td>67.1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>wichita-state</td>\n",
       "      <td>1891</td>\n",
       "      <td>0.407</td>\n",
       "      <td>1147</td>\n",
       "      <td>0.458</td>\n",
       "      <td>744</td>\n",
       "      <td>0.328</td>\n",
       "      <td>638</td>\n",
       "      <td>0.697</td>\n",
       "      <td>374</td>\n",
       "      <td>...</td>\n",
       "      <td>219</td>\n",
       "      <td>115</td>\n",
       "      <td>396</td>\n",
       "      <td>577</td>\n",
       "      <td>71.8</td>\n",
       "      <td>0.742</td>\n",
       "      <td>13.67</td>\n",
       "      <td>5.61</td>\n",
       "      <td>63.8</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>utah-state</td>\n",
       "      <td>1967</td>\n",
       "      <td>0.460</td>\n",
       "      <td>1222</td>\n",
       "      <td>0.538</td>\n",
       "      <td>745</td>\n",
       "      <td>0.333</td>\n",
       "      <td>710</td>\n",
       "      <td>0.763</td>\n",
       "      <td>366</td>\n",
       "      <td>...</td>\n",
       "      <td>209</td>\n",
       "      <td>109</td>\n",
       "      <td>426</td>\n",
       "      <td>532</td>\n",
       "      <td>76.5</td>\n",
       "      <td>0.765</td>\n",
       "      <td>13.66</td>\n",
       "      <td>4.04</td>\n",
       "      <td>64.1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>north-carolina-central</td>\n",
       "      <td>1677</td>\n",
       "      <td>0.459</td>\n",
       "      <td>1094</td>\n",
       "      <td>0.547</td>\n",
       "      <td>583</td>\n",
       "      <td>0.295</td>\n",
       "      <td>556</td>\n",
       "      <td>0.644</td>\n",
       "      <td>294</td>\n",
       "      <td>...</td>\n",
       "      <td>251</td>\n",
       "      <td>88</td>\n",
       "      <td>458</td>\n",
       "      <td>566</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.567</td>\n",
       "      <td>-10.25</td>\n",
       "      <td>-8.40</td>\n",
       "      <td>65.7</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Team   FGA    FG%   2PA    2P%  3PA    3P%  FTA    FT%  \\\n",
       "0                   kansas  1758  0.484  1180  0.553  578  0.344  616  0.667   \n",
       "1          san-diego-state  1820  0.468  1055  0.532  765  0.380  519  0.771   \n",
       "2                 maryland  1774  0.419  1009  0.501  765  0.311  669  0.747   \n",
       "3                 kentucky  1723  0.461  1265  0.504  458  0.343  703  0.797   \n",
       "4                   butler  1687  0.460  1066  0.529  621  0.341  496  0.728   \n",
       "5                   auburn  1868  0.433  1044  0.533  824  0.306  818  0.674   \n",
       "6            west-virginia  1833  0.422  1291  0.479  542  0.286  742  0.642   \n",
       "7           saint-marys-ca  1879  0.480  1199  0.529  680  0.393  564  0.752   \n",
       "8                 oklahoma  1808  0.420  1104  0.486  704  0.318  565  0.766   \n",
       "9               texas-tech  1724  0.452  1093  0.508  631  0.355  594  0.754   \n",
       "10           arizona-state  1904  0.429  1201  0.490  703  0.323  617  0.695   \n",
       "11                  purdue  1867  0.418  1192  0.467  675  0.330  482  0.687   \n",
       "12                richmond  1817  0.466  1129  0.530  688  0.362  505  0.776   \n",
       "13               tennessee  1656  0.432  1058  0.499  598  0.313  617  0.749   \n",
       "14            rhode-island  1851  0.423  1312  0.467  539  0.315  686  0.676   \n",
       "15       mississippi-state  1713  0.468  1234  0.519  479  0.336  676  0.754   \n",
       "16                 gonzaga  1977  0.514  1355  0.573  622  0.387  765  0.688   \n",
       "17           florida-state  1848  0.457  1193  0.515  655  0.351  557  0.750   \n",
       "18              louisville  1787  0.453  1108  0.501  679  0.376  576  0.731   \n",
       "19               villanova  1806  0.441   952  0.515  854  0.359  485  0.759   \n",
       "20              penn-state  1931  0.431  1166  0.500  765  0.325  598  0.692   \n",
       "21                    iowa  1868  0.451  1165  0.513  703  0.347  643  0.750   \n",
       "22                 arizona  1847  0.448  1210  0.498  637  0.352  670  0.731   \n",
       "23                 houston  1868  0.422  1227  0.466  641  0.337  615  0.730   \n",
       "24         louisiana-state  1880  0.471  1235  0.547  645  0.327  666  0.769   \n",
       "25                  xavier  1852  0.435  1205  0.500  647  0.315  576  0.656   \n",
       "26                   texas  1730  0.428   988  0.498  742  0.334  381  0.685   \n",
       "27                   tulsa  1652  0.441  1086  0.503  566  0.322  657  0.708   \n",
       "28                 vermont  1819  0.446  1031  0.535  788  0.330  555  0.714   \n",
       "29                 belmont  2074  0.475  1166  0.571  908  0.352  470  0.713   \n",
       "..                     ...   ...    ...   ...    ...  ...    ...  ...    ...   \n",
       "34                    duke  1957  0.470  1340  0.525  617  0.352  696  0.720   \n",
       "35                  oregon  1825  0.470  1145  0.514  680  0.396  527  0.700   \n",
       "36           brigham-young  1897  0.500  1112  0.555  785  0.423  454  0.700   \n",
       "37                colorado  1721  0.439  1055  0.492  666  0.356  611  0.738   \n",
       "38                illinois  1831  0.443  1265  0.503  566  0.309  607  0.728   \n",
       "39                 indiana  1759  0.442  1241  0.492  518  0.322  699  0.681   \n",
       "40              providence  1852  0.413  1174  0.459  678  0.332  664  0.694   \n",
       "41                 florida  1729  0.457  1093  0.522  636  0.347  588  0.723   \n",
       "42           northern-iowa  1776  0.475  1039  0.536  737  0.389  468  0.759   \n",
       "43                 liberty  1754  0.479  1044  0.557  710  0.366  548  0.712   \n",
       "44                   akron  1789  0.440   994  0.504  795  0.361  629  0.771   \n",
       "45                    yale  1726  0.469  1012  0.538  714  0.371  570  0.712   \n",
       "46                 radford  1813  0.464  1106  0.533  707  0.355  575  0.642   \n",
       "47                 hofstra  1934  0.451  1206  0.498  728  0.374  652  0.779   \n",
       "48                  dayton  1725  0.525  1053  0.623  672  0.371  585  0.720   \n",
       "49          michigan-state  1833  0.461  1146  0.529  687  0.348  565  0.750   \n",
       "50               creighton  1839  0.471  1056  0.534  783  0.386  533  0.739   \n",
       "51              ohio-state  1690  0.446   990  0.497  700  0.373  621  0.752   \n",
       "52               wisconsin  1718  0.425   954  0.483  764  0.352  462  0.764   \n",
       "53                michigan  1864  0.462  1135  0.541  729  0.339  498  0.717   \n",
       "54                virginia  1515  0.413   955  0.477  560  0.304  403  0.717   \n",
       "55               marquette  1764  0.430   977  0.469  787  0.382  692  0.743   \n",
       "56     southern-california  1780  0.439  1209  0.481  571  0.350  630  0.662   \n",
       "57                 rutgers  1847  0.447  1293  0.505  554  0.310  537  0.644   \n",
       "58    east-tennessee-state  1936  0.475  1184  0.557  752  0.346  713  0.694   \n",
       "59        stephen-f-austin  1806  0.491  1312  0.540  494  0.362  790  0.691   \n",
       "60              cincinnati  1713  0.445  1095  0.517  618  0.319  678  0.677   \n",
       "61           wichita-state  1891  0.407  1147  0.458  744  0.328  638  0.697   \n",
       "62              utah-state  1967  0.460  1222  0.538  745  0.333  710  0.763   \n",
       "63  north-carolina-central  1677  0.459  1094  0.547  583  0.295  556  0.644   \n",
       "\n",
       "    ORB  ...  STL  BLK  TOV   PF  PTS/G   W-L%    SRS    SOS  PTS.1  Seed  \n",
       "0   333  ...  237  145  396  454   74.6  0.903  25.01  11.68   60.7     1  \n",
       "1   297  ...  221   88  344  534   74.8  0.938  18.35   3.90   59.5     2  \n",
       "2   348  ...  142  134  350  463   71.8  0.774  17.82  10.49   64.5     3  \n",
       "3   300  ...  162  153  387  568   74.4  0.806  15.72   7.40   66.1     4  \n",
       "4   280  ...  165   82  349  519   68.5  0.710  15.59   9.17   62.1     5  \n",
       "5   421  ...  199  151  399  596   78.0  0.806  15.01   7.36   70.4     6  \n",
       "6   470  ...  238  111  447  572   70.3  0.677  18.49  10.62   62.4     7  \n",
       "7   257  ...  224   66  325  544   75.5  0.788  12.48   4.67   66.4     8  \n",
       "8   259  ...  190  135  345  415   70.2  0.613  13.50  10.67   67.4     9  \n",
       "9   279  ...  239  124  423  545   71.9  0.581  16.81   8.39   63.5    10  \n",
       "10  312  ...  249  104  408  577   73.8  0.645  11.41   8.06   70.5    11  \n",
       "11  394  ...  202  109  357  544   68.2  0.516  16.30  10.75   62.6    12  \n",
       "12  234  ...  252   78  338  479   75.3  0.774  11.40   2.85   66.8    13  \n",
       "13  311  ...  186  174  423  563   67.1  0.548  11.18   8.31   64.2    14  \n",
       "14  359  ...  252  116  355  519   73.3  0.700   9.25   4.78   68.9    15  \n",
       "15  371  ...  174  158  431  542   73.4  0.645  12.20   6.95   68.1    16  \n",
       "16  362  ...  238  123  362  467   87.5  0.938  22.50   2.88   67.9     1  \n",
       "17  350  ...  273  162  410  542   75.4  0.839  17.27   7.63   65.7     2  \n",
       "18  335  ...  157   83  376  524   74.1  0.774  17.93   7.55   63.7     3  \n",
       "19  271  ...  175   81  335  471   73.2  0.774  16.84  10.03   66.4     4  \n",
       "20  348  ...  233  150  348  543   75.1  0.677  16.12   9.25   68.2     5  \n",
       "21  356  ...  191  120  375  496   77.7  0.645  16.05  10.59   72.3     6  \n",
       "22  338  ...  207  115  365  572   76.4  0.645  19.46   8.24   65.2     7  \n",
       "23  447  ...  161  151  369  608   72.3  0.742  16.47   6.31   62.1     8  \n",
       "24  375  ...  195  121  390  495   80.5  0.677  14.41   7.18   73.3     9  \n",
       "25  371  ...  191  113  435  521   70.8  0.613  12.85   9.53   67.5    10  \n",
       "26  278  ...  171  134  420  568   64.2  0.613  10.46   9.59   63.3    11  \n",
       "27  268  ...  234   70  417  469   67.8  0.677   7.93   3.60   63.5    12  \n",
       "28  308  ...  189  109  363  518   71.3  0.781   6.28  -4.76   59.0    13  \n",
       "29  290  ...  249  106  392  497   79.6  0.788   4.31  -6.22   67.9    14  \n",
       "..  ...  ...  ...  ...  ...  ...    ...    ...    ...    ...    ...   ...  \n",
       "34  394  ...  258  188  409  557   82.5  0.806  22.60   8.06   68.0     3  \n",
       "35  336  ...  241  102  368  538   75.9  0.774  18.23   9.10   66.7     4  \n",
       "36  209  ...  186   67  353  513   79.6  0.750  16.97   6.49   68.4     5  \n",
       "37  312  ...  179   94  411  451   71.0  0.677  15.64   8.38   63.7     6  \n",
       "38  393  ...  151   89  375  497   72.2  0.677  15.30   9.86   65.3     7  \n",
       "39  353  ...  175  128  393  532   70.9  0.613  14.67  10.61   66.8     8  \n",
       "40  406  ...  257  102  417  581   71.4  0.613  13.47   8.21   66.2     9  \n",
       "41  313  ...  178  121  369  528   71.9  0.613  14.94   9.10   66.0    10  \n",
       "42  294  ...  143   71  372  443   75.2  0.806   9.86   1.49   64.7    11  \n",
       "43  250  ...  192   96  318  456   68.6  0.882   5.17  -7.24   53.8    12  \n",
       "44  302  ...  158   91  397  548   75.7  0.774   7.11  -0.44   66.5    13  \n",
       "45  273  ...  188  101  389  449   76.3  0.767   6.62  -1.53   65.1    14  \n",
       "46  340  ...  170   68  342  561   71.9  0.656  -2.66  -5.70   66.1    15  \n",
       "47  339  ...  231   70  374  475   76.5  0.758   2.88  -3.97   68.6    16  \n",
       "48  242  ...  176  109  377  529   80.0  0.935  19.22   3.74   64.5     1  \n",
       "49  344  ...  159  156  389  524   75.9  0.710  21.92  10.76   64.7     2  \n",
       "50  246  ...  203   79  337  472   78.3  0.774  17.25   9.61   69.7     3  \n",
       "51  308  ...  156  107  394  531   72.1  0.677  19.37  10.24   62.9     4  \n",
       "52  251  ...  138  114  309  476   67.2  0.677  15.96  10.96   62.2     5  \n",
       "53  267  ...  169  140  332  470   75.0  0.613  17.96  11.19   68.3     6  \n",
       "54  243  ...  161  147  368  397   57.0  0.767  12.19   7.53   52.4     7  \n",
       "55  317  ...  154  127  410  572   77.8  0.600  15.41   9.27   71.6     8  \n",
       "56  333  ...  207  152  448  519   70.3  0.710  11.52   7.23   66.0     9  \n",
       "57  365  ...  203  142  379  520   69.9  0.645  15.73   9.69   62.6    10  \n",
       "58  370  ...  278  121  412  598   76.3  0.882   7.63  -1.82   63.1    11  \n",
       "59  406  ...  324  101  550  649   80.6  0.903   4.00  -6.93   67.0    12  \n",
       "60  329  ...  196  146  432  540   72.7  0.667  12.37   6.77   67.1    13  \n",
       "61  374  ...  219  115  396  577   71.8  0.742  13.67   5.61   63.8    14  \n",
       "62  366  ...  209  109  426  532   76.5  0.765  13.66   4.04   64.1    15  \n",
       "63  294  ...  251   88  458  566   69.0  0.567 -10.25  -8.40   65.7    16  \n",
       "\n",
       "[64 rows x 22 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tournament_stats.rename(columns = {'opp_PPG': 'PTS.1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FGA', 'FG%', '2PA', '2P%', '3PA', '3P%', 'FTA', 'FT%', 'ORB', 'DRB',\n",
       "       'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS/G', 'W-L%', 'SRS', 'SOS',\n",
       "       'PTS.1', 'Seed', 'result'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_matrix.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "* https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680\n",
    "* Rafael Gomes Mantovani, Tom Horvth, Ricardo Cerri, Sylvio Barbon Junior, Joaquin Vanschoren, Andr Carlos Ponce de Leon Ferreira de Carvalho, An empirical study on hyperparameter tuning of decision trees arXiv:1812.02207 \" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the hyperparameters we will tune here\n",
    "depth = 13     # 13 is the default\n",
    "mss = 20       # best range for CART = 1 - 40\n",
    "msl = 15       # best range for CART = 1-20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split the data\n",
    "Y = data_matrix.iloc[: , -1]\n",
    "X = data_matrix.iloc[: , :-1]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "# Build Classifier\n",
    "model = tree.DecisionTreeClassifier(max_depth = depth , min_samples_split = mss , min_samples_leaf = msl)\n",
    "\n",
    "# fit the classifier\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "#make predictions\n",
    "predicted_results = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(model.tree_.max_depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8380414312617702"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7894736842105263"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x_test , y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression \n",
    "in this section, we will assume there exists a linear relationship y = wx. We want to optimize. <dir> First, lets create a correlation matrix and see what linear relationships exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "corr=data_matrix.corr()\n",
    "ax=sns.heatmap(corr,annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notable results: <dir>\n",
    "    * defensive rebounds are more correlated with wins than offensive rebounds\n",
    "    * almost no correlation between FT% and Wins in NCAA tournament!\n",
    "    * low correlation between 3P% and 3PA \n",
    "        + Perhaps this is something that can be accounted for.\n",
    "        + Feature engineering could be useful - see what percentage of shots were 3P "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nueral Net\n",
    "\n",
    "This is adapted from the TF example https://www.tensorflow.org/tutorials/keras/classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "nueral_net_data = data_matrix.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split the data\n",
    "Y_nn = nueral_net_data.iloc[: , -1]\n",
    "X_nn = nueral_net_data.iloc[: , :-1]\n",
    "x_train_nn, x_test_nn, y_train_nn, y_test_nn = train_test_split(X_nn, Y_nn, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Nueral Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is built using 256 hidden units and a sigmoid activation function\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_shape = (21,)  # The shape of the data. This cannot be changed.\n",
    "num_categories = 2 # The number of labelled categories in our classification scheme. This cannot be changed.\n",
    "hidden_units_list = [16, 64, 256]\n",
    "\n",
    "# These are our layers\n",
    "num_hidden_units = hidden_units_list[2] \n",
    "num_hidden_units_2 = hidden_units_list[2]\n",
    "num_hidden_units_3 = hidden_units_list[2]\n",
    "activation_list = ['sigmoid', 'relu', 'tanh'] # Different options of activation function\n",
    "activation = activation_list[0]\n",
    "\n",
    "nueral_net = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=input_shape),\n",
    "    keras.layers.Dense(num_hidden_units, activation=activation),\n",
    "    keras.layers.Dropout(0.5) ,\n",
    "    keras.layers.Dense(num_hidden_units_2, activation=activation),\n",
    "    keras.layers.Dropout(0.25) ,\n",
    "\n",
    "    keras.layers.Dense(num_categories, activation='softmax')\n",
    "])\n",
    "\n",
    "print(f\"The model is built using {num_hidden_units} hidden units and a {activation} activation function\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 531 samples\n",
      "Epoch 1/30\n",
      "531/531 [==============================] - 1s 1ms/sample - loss: 0.6213 - accuracy: 0.6667\n",
      "Epoch 2/30\n",
      "531/531 [==============================] - 0s 85us/sample - loss: 0.5488 - accuracy: 0.7420\n",
      "Epoch 3/30\n",
      "531/531 [==============================] - 0s 83us/sample - loss: 0.5398 - accuracy: 0.7571\n",
      "Epoch 4/30\n",
      "531/531 [==============================] - 0s 83us/sample - loss: 0.5328 - accuracy: 0.7608\n",
      "Epoch 5/30\n",
      "531/531 [==============================] - 0s 84us/sample - loss: 0.5202 - accuracy: 0.7797\n",
      "Epoch 6/30\n",
      "531/531 [==============================] - 0s 91us/sample - loss: 0.5124 - accuracy: 0.7966\n",
      "Epoch 7/30\n",
      "531/531 [==============================] - 0s 85us/sample - loss: 0.5168 - accuracy: 0.7853\n",
      "Epoch 8/30\n",
      "531/531 [==============================] - 0s 81us/sample - loss: 0.5070 - accuracy: 0.8004\n",
      "Epoch 9/30\n",
      "531/531 [==============================] - 0s 81us/sample - loss: 0.5114 - accuracy: 0.7910\n",
      "Epoch 10/30\n",
      "531/531 [==============================] - 0s 83us/sample - loss: 0.4950 - accuracy: 0.8136\n",
      "Epoch 11/30\n",
      "531/531 [==============================] - 0s 82us/sample - loss: 0.4944 - accuracy: 0.8041\n",
      "Epoch 12/30\n",
      "531/531 [==============================] - 0s 75us/sample - loss: 0.4873 - accuracy: 0.8211\n",
      "Epoch 13/30\n",
      "531/531 [==============================] - 0s 88us/sample - loss: 0.4924 - accuracy: 0.8154\n",
      "Epoch 14/30\n",
      "531/531 [==============================] - 0s 83us/sample - loss: 0.4928 - accuracy: 0.8079\n",
      "Epoch 15/30\n",
      "531/531 [==============================] - 0s 75us/sample - loss: 0.4873 - accuracy: 0.8230\n",
      "Epoch 16/30\n",
      "531/531 [==============================] - 0s 81us/sample - loss: 0.4791 - accuracy: 0.8305\n",
      "Epoch 17/30\n",
      "531/531 [==============================] - 0s 77us/sample - loss: 0.4810 - accuracy: 0.8286\n",
      "Epoch 18/30\n",
      "531/531 [==============================] - 0s 83us/sample - loss: 0.4789 - accuracy: 0.8343\n",
      "Epoch 19/30\n",
      "531/531 [==============================] - 0s 76us/sample - loss: 0.4722 - accuracy: 0.8399\n",
      "Epoch 20/30\n",
      "531/531 [==============================] - 0s 77us/sample - loss: 0.4679 - accuracy: 0.8418\n",
      "Epoch 21/30\n",
      "531/531 [==============================] - 0s 75us/sample - loss: 0.4663 - accuracy: 0.8512\n",
      "Epoch 22/30\n",
      "531/531 [==============================] - 0s 75us/sample - loss: 0.4661 - accuracy: 0.8399\n",
      "Epoch 23/30\n",
      "531/531 [==============================] - 0s 80us/sample - loss: 0.4588 - accuracy: 0.8512\n",
      "Epoch 24/30\n",
      "531/531 [==============================] - 0s 75us/sample - loss: 0.4507 - accuracy: 0.8663\n",
      "Epoch 25/30\n",
      "531/531 [==============================] - 0s 76us/sample - loss: 0.4556 - accuracy: 0.8531\n",
      "Epoch 26/30\n",
      "531/531 [==============================] - 0s 77us/sample - loss: 0.4480 - accuracy: 0.8682\n",
      "Epoch 27/30\n",
      "531/531 [==============================] - 0s 77us/sample - loss: 0.4475 - accuracy: 0.8682\n",
      "Epoch 28/30\n",
      "531/531 [==============================] - 0s 76us/sample - loss: 0.4579 - accuracy: 0.8456\n",
      "Epoch 29/30\n",
      "531/531 [==============================] - 0s 76us/sample - loss: 0.4473 - accuracy: 0.8644\n",
      "Epoch 30/30\n",
      "531/531 [==============================] - 0s 80us/sample - loss: 0.4494 - accuracy: 0.8663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14baea5c0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adam helps get out of local optima\n",
    "nueral_net.compile(optimizer='adam', # adam is an adaptive gradient optimizer\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "nueral_net.fit(x_train_nn, y_train_nn, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 - 0s - loss: 0.5358 - accuracy: 0.7594\n",
      "\n",
      "Test accuracy: 0.7593985\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = nueral_net.evaluate(x_test_nn,  y_test_nn, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the tournament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# to do:\n",
    "    # fill out this list\n",
    "    # decide what framework the teams should be in. \n",
    "\n",
    "ncaa_teams = {\n",
    "    'south' : [\n",
    "        'kansas' , \n",
    "        'san-diego-state' , \n",
    "        'maryland' , \n",
    "        'kentucky' , \n",
    "        'butler' , \n",
    "        'auburn' , \n",
    "        'west-virginia' , \n",
    "        'saint-marys-ca' , \n",
    "        'oklahoma' , \n",
    "        'texas-tech' , \n",
    "        'arizona-state' , \n",
    "        'purdue' , \n",
    "        'richmond' , \n",
    "        'tennessee' , \n",
    "        'rhode-island' , \n",
    "        'mississippi-state' \n",
    "        ] , \n",
    "        'east' : [\n",
    "        'gonzaga' , \n",
    "        'florida-state' , \n",
    "        'louisville' , \n",
    "        'villanova' , \n",
    "        'penn-state' , \n",
    "        'iowa' , \n",
    "        'arizona' , \n",
    "        'houston' , \n",
    "        'louisiana-state' , \n",
    "        'xavier' , \n",
    "        'texas' , \n",
    "        'tulsa' , \n",
    "        'vermont' , \n",
    "        'belmont' , \n",
    "        'wright-state' , \n",
    "        'siena' \n",
    "        ]  ,\n",
    "        'midwest' : [\n",
    "        'baylor' , \n",
    "        'seton-hall' , \n",
    "        'duke' , \n",
    "        'oregon' , \n",
    "        'brigham-young' ,  # byu\n",
    "        'colorado' , \n",
    "        'illinois' , \n",
    "        'indiana' , \n",
    "        'providence' , \n",
    "        'florida' , \n",
    "        'northern-iowa' , \n",
    "        'liberty' , \n",
    "        'akron' , \n",
    "        'yale' , \n",
    "        'radford' , \n",
    "        'hofstra' \n",
    "        ]  ,\n",
    "        'west' : [\n",
    "        'dayton' , \n",
    "        'michigan-state' , \n",
    "        'creighton' , \n",
    "        'ohio-state' , \n",
    "        'wisconsin' , \n",
    "        'michigan' , \n",
    "        'virginia' , \n",
    "        'marquette' , \n",
    "        'southern-california' , \n",
    "        'rutgers' , \n",
    "        'east-tennessee-state' , \n",
    "        'stephen-f-austin' , \n",
    "        'cincinnati' , \n",
    "        'wichita-state' , \n",
    "        'utah-state' , \n",
    "        'north-carolina-central' \n",
    "        ]\n",
    "    \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## build the tournament\n",
    "\n",
    "class tournament():\n",
    "    \n",
    "   \n",
    "    def __init__(self , teams , model ):\n",
    "        self.origional = teams\n",
    "        self.teams = copy.deepcopy(teams)               # this needs to be a list of dictionary's south , west, midwest , east, teams in seed order\n",
    "        self.games = {}\n",
    "        self.rounds = []\n",
    "        self.round_32 = []\n",
    "        self.round_16 = []\n",
    "        self.round_8 = []\n",
    "        self.final_four = [ 0 , 0 , 0 , 0]\n",
    "        self.finals = None\n",
    "        self.champion = None\n",
    "        self.model = model\n",
    "         \n",
    "    # this method will play the entire tournament by alternating playing make_games and play_round\n",
    "    def play_tournament(self):\n",
    "        while self.champion == None:\n",
    "            self.make_games()\n",
    "            self.play_round()\n",
    "\n",
    "    # this method resets the tournament, but uses the origional set of teams.\n",
    "    def reset(self):\n",
    "        self.teams = self.origional\n",
    "        self.games = {}\n",
    "        self.rounds = []\n",
    "        self.round_32 = []\n",
    "        self.round_16 = []\n",
    "        self.round_8 = []\n",
    "        self.final_four = [ 0 , 0 , 0 , 0]\n",
    "        self.finals = None\n",
    "        self.champion = None\n",
    "        self.model = model\n",
    "        \n",
    "        \n",
    "    # this method builds the next round of games using the results of the play_round() method\n",
    "    def make_games(self):\n",
    "        # this makes all the games for each region - if only one team left: it will return no games\n",
    "        for region in self.teams:\n",
    "            reg_teams = self.teams[region] # this get's the teams and puts them into a list\n",
    "            games = []\n",
    "            for i in range(len(reg_teams) // 2):\n",
    "                games += [game(reg_teams[i] , reg_teams[(i + 1) * -1]  , model = self.model , team_data = tournament_stats)]\n",
    "            self.games[region] = games\n",
    "\n",
    "    # The following method will play a single round of the tournamnet\n",
    "    # it is important to note that it considers the final four, and championship as one round\n",
    "    # At the conclusion, it sets the self.teams to be just the winners. keeping track of each\n",
    "    # round in self.rounds\n",
    "    # the final four / finals round returns self.champion as the overall winner\n",
    "    def play_round(self):\n",
    "        \n",
    "        games_played = []\n",
    "        counter = 0        \n",
    "        for region in self.games:\n",
    "            l_games = list(self.games[region])\n",
    "            l_teams = list(self.teams[region])\n",
    "            if len(l_teams) != 1:\n",
    "                winners = []\n",
    "                \n",
    "                for g in l_games:\n",
    "                    g.play_model()\n",
    "                    #game.print_results\n",
    "                    winners += [g.winner]\n",
    "                    games_played += [g]\n",
    "                # update the teams in the tournament\n",
    "                self.teams[region] = winners\n",
    "\n",
    "            else:\n",
    "                # make the final four\n",
    "                self.final_four[counter] = self.teams[region][0]\n",
    "                # play the final four  # this needs to be the correct pairing\n",
    "                        # update the counter\n",
    "            counter += 1\n",
    "        # update the rounds\n",
    "        if games_played != []:\n",
    "            self.rounds += [games_played] \n",
    "        if self.final_four[0] != 0:\n",
    "            game1 = game(self.final_four[0] , self.final_four[1], model = self.model , team_data = tournament_stats)\n",
    "            game1.play_model()\n",
    "            game2 = game(self.final_four[2] , self.final_four[3], model = self.model , team_data = tournament_stats)\n",
    "            game2.play_model()\n",
    "            \n",
    "            # put winners in finals \n",
    "            self.finals = [game1.winner , game2.winner]\n",
    "                \n",
    "            self.rounds += [ [game1 , game2] ]\n",
    "                \n",
    "            # play the final\n",
    "            final_game = game(self.finals[0] , self.finals[1], model = self.model , team_data = tournament_stats)\n",
    "                \n",
    "            final_game.play_model()\n",
    "            self.champion = final_game.winner\n",
    "            self.rounds += [[final_game]]\n",
    "\n",
    "\n",
    "    def short_describe(self):\n",
    "        final_four = self.rounds[-2]\n",
    "        finals = self.rounds[-1]\n",
    "        \n",
    "        print(\"In the Final Four,  \")\n",
    "        for game in final_four:\n",
    "            game.print_results()\n",
    "        print(\"In the finals,  \")\n",
    "        finals[-1].print_results()\n",
    "        \n",
    "    def give_results(self):\n",
    "        round_count = 1\n",
    "        for r in self.rounds:\n",
    "            print(\"In round \" , round_count)\n",
    "            round_count += 1\n",
    "            for game in r:\n",
    "                game.print_results()\n",
    "            print(\"   \")\n",
    "            \n",
    "        \n",
    "        \n",
    "class game():\n",
    "    \n",
    "    def __init__(self, team1 , team2 , model = None , team_data = None):\n",
    "        self.team_one = team1\n",
    "        self.team_two = team2\n",
    "        self.model = model\n",
    "        self.team_data = team_data\n",
    "        self.winner = None\n",
    "        self.loser = None\n",
    "        \n",
    "\n",
    "    def print_results(self):\n",
    "        print(self.winner , \" will beat \" , self.loser)\n",
    "        \n",
    "    # this method plays a game assuming each team has equal chance of winning. \n",
    "    def play_even(self):\n",
    "        choice = np.random.uniform()\n",
    "        if choice <= 0.5:\n",
    "            self.winner = self.team_one\n",
    "            self.loser  = self.team_two\n",
    "        else:\n",
    "            self.winner = self.team_two\n",
    "            self.loser  = self.team_one\n",
    "            \n",
    "    def play_model(self):\n",
    "        model = self.model\n",
    "        # need to build the feature vector\n",
    "        s1 = self.team_data[self.team_data[\"Team\"] == self.team_one ]\n",
    "        s2 = self.team_data[self.team_data[\"Team\"] == self.team_two ]\n",
    "        s1 = s1.drop([\"Team\" ] , axis = 1)\n",
    "        s2 = s2.drop([\"Team\" ] , axis = 1)\n",
    "        # need to reset their indices\n",
    "        s2 = s2.reset_index(drop=True)\n",
    "        s1 = s1.reset_index(drop=True)   \n",
    "        # subtract the two stat sets\n",
    "        feature_vector = s1.sub(s2) \n",
    "        # get prediction\n",
    "        choice = model.predict(feature_vector)   # if you import decision tree\n",
    "        #choice = model.predict_classes(feature_vector)  # if you import nueral net\n",
    "        if choice == 1:\n",
    "            self.winner = self.team_one\n",
    "            self.loser  = self.team_two\n",
    "        else:\n",
    "            self.winner = self.team_two\n",
    "            self.loser  = self.team_one\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the Final Four,  \n",
      "san-diego-state  will beat  gonzaga\n",
      "east-tennessee-state  will beat  duke\n",
      "In the finals,  \n",
      "east-tennessee-state  will beat  san-diego-state\n",
      "In round  1\n",
      "kansas  will beat  mississippi-state\n",
      "san-diego-state  will beat  rhode-island\n",
      "maryland  will beat  tennessee\n",
      "richmond  will beat  kentucky\n",
      "purdue  will beat  butler\n",
      "auburn  will beat  arizona-state\n",
      "west-virginia  will beat  texas-tech\n",
      "saint-marys-ca  will beat  oklahoma\n",
      "gonzaga  will beat  siena\n",
      "wright-state  will beat  florida-state\n",
      "belmont  will beat  louisville\n",
      "villanova  will beat  vermont\n",
      "penn-state  will beat  tulsa\n",
      "iowa  will beat  texas\n",
      "arizona  will beat  xavier\n",
      "houston  will beat  louisiana-state\n",
      "hofstra  will beat  baylor\n",
      "seton-hall  will beat  radford\n",
      "duke  will beat  yale\n",
      "oregon  will beat  akron\n",
      "liberty  will beat  brigham-young\n",
      "colorado  will beat  northern-iowa\n",
      "illinois  will beat  florida\n",
      "providence  will beat  indiana\n",
      "dayton  will beat  north-carolina-central\n",
      "utah-state  will beat  michigan-state\n",
      "creighton  will beat  wichita-state\n",
      "ohio-state  will beat  cincinnati\n",
      "wisconsin  will beat  stephen-f-austin\n",
      "east-tennessee-state  will beat  michigan\n",
      "virginia  will beat  rutgers\n",
      "southern-california  will beat  marquette\n",
      "   \n",
      "In round  2\n",
      "saint-marys-ca  will beat  kansas\n",
      "san-diego-state  will beat  west-virginia\n",
      "auburn  will beat  maryland\n",
      "richmond  will beat  purdue\n",
      "gonzaga  will beat  houston\n",
      "arizona  will beat  wright-state\n",
      "iowa  will beat  belmont\n",
      "penn-state  will beat  villanova\n",
      "providence  will beat  hofstra\n",
      "illinois  will beat  seton-hall\n",
      "duke  will beat  colorado\n",
      "liberty  will beat  oregon\n",
      "dayton  will beat  southern-california\n",
      "utah-state  will beat  virginia\n",
      "east-tennessee-state  will beat  creighton\n",
      "ohio-state  will beat  wisconsin\n",
      "   \n",
      "In round  3\n",
      "saint-marys-ca  will beat  richmond\n",
      "san-diego-state  will beat  auburn\n",
      "gonzaga  will beat  penn-state\n",
      "arizona  will beat  iowa\n",
      "liberty  will beat  providence\n",
      "duke  will beat  illinois\n",
      "dayton  will beat  ohio-state\n",
      "east-tennessee-state  will beat  utah-state\n",
      "   \n",
      "In round  4\n",
      "san-diego-state  will beat  saint-marys-ca\n",
      "gonzaga  will beat  arizona\n",
      "duke  will beat  liberty\n",
      "east-tennessee-state  will beat  dayton\n",
      "   \n",
      "In round  5\n",
      "san-diego-state  will beat  gonzaga\n",
      "east-tennessee-state  will beat  duke\n",
      "   \n",
      "In round  6\n",
      "east-tennessee-state  will beat  san-diego-state\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "t = tournament(ncaa_teams , model)\n",
    "t.play_tournament()\n",
    "t.short_describe()\n",
    "t.give_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
