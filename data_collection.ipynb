{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MARCH MADNESS PREDICTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Connor Finn, Riley Greene <br>\n",
    "Date: 1/24/20 <br>\n",
    "Warren Buffet is still paying 1 billion for a perfect bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our imports for the model\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import lxml.html as lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class will create the data pipeline for this project. It has the following methods:\n",
    "* make_folder() takes in no external arguments, it simply creates a folder in the working directory called 'collected_data' if one does not already exist.\n",
    "* write_data_one() takes in a bball_scraper object (defined below). \n",
    "  + It writes the scraped data into a csv file. \n",
    "* write_data_two() works identically to write_data_one but has a different end csv file\n",
    "* write_team_list() takes in a dataframe and writes it to csv.\n",
    "* write_game_list() takes in a dataframe and writes it to csv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class collect_pipeline(object):\n",
    "\n",
    "    def make_folder(self):\n",
    "        if not os.path.exists('./collected_data'):\n",
    "            os.mkdir('./collected_data')\n",
    "\n",
    "    def write_data_one(self, spider):\n",
    "        spider.data.to_csv('collected_data/season_data.csv' , index=False )\n",
    "    \n",
    "    def write_data_two(self , spider):\n",
    "        spider.data.to_csv('collected_data/more_team_data.csv' , index=False)\n",
    "        \n",
    "    def write_team_list(self, df):\n",
    "        df.to_csv('collected_data/team_list.csv')\n",
    "    \n",
    "    def write_game_list(self, df):\n",
    "        df.to_csv('collected_data/ncaa_short.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Spider\n",
    "\n",
    "This class defines the web scraper. The goal is to scrape sports-reference.com webpages in search of two tables for each team. The tables will give us a lot of season long data for each team. This is useful, because this is the information that we will have access to before making our predictions for the NCAA tournament. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class bball_scraper():\n",
    "   \n",
    "\n",
    "    def __init__(self ):\n",
    "        self.start_urls = []\n",
    "        self.data_one = None\n",
    "    \n",
    "    # url_list is a list of dictionary's. each has a 'name' id' , 'year' , 'url'\n",
    "    # for the second group of urls - data will be none type\n",
    "    def set_urls(self , url_list):\n",
    "        self.start_urls = url_list\n",
    "    \n",
    "    # this is the first set of urls. only need the first url from one table\n",
    "    def parse_one(self):\n",
    "        season_stats = pd.DataFrame()   # start with an empty dataframe\n",
    "        \n",
    "        for item in self.start_urls:\n",
    "            # extact the information we need\n",
    "            team_id = item['id']\n",
    "            team_name = item['name']\n",
    "            url = item['url']\n",
    "            year = item['year']\n",
    "            page = requests.get(url)\n",
    "            soup = BeautifulSoup(page.content, \"lxml\")\n",
    "            tables = soup.findAll('table')    # search for tables \n",
    "            # convert to a dataframe and label the data\n",
    "            df = pd.read_html(str(tables[1]))[0]      # select the table of interest into a pandas dataframe  \n",
    "            df.drop([1 , 2, 3] , inplace=True)\n",
    "            df = df.replace('Team' , team_name)   # want the name of the team \n",
    "            df['Team_ID'] = team_id        # put in the team ID's\n",
    "            df['Date'] = year  # only want the year in this column\n",
    "            df = df.rename(columns={'Unnamed: 0': 'Team'})\n",
    "\n",
    "            \n",
    "            season_stats = season_stats.append(df , ignore_index = True , sort=False)\n",
    "           # season_stats = pd.concat([season_stats, df] ,  axis=0) # add this to the season stats empty dataframe we started with\n",
    "              # simply clean up the columns \n",
    "            \n",
    "            # save the data\n",
    "            self.data = season_stats  \n",
    "            \n",
    "        # this is the second set of urls - more than one row from the table\n",
    "    def parse_two(self):\n",
    "        wl_teams = pd.DataFrame()      # initialize and empty dataframe\n",
    "        for item in self.start_urls:\n",
    "                # extract the info\n",
    "            team_idea = item['id']\n",
    "            team_name = item['name']\n",
    "            url = item['url']\n",
    "                   \n",
    "                # parse the page\n",
    "            page = requests.get(url)\n",
    "            soup = BeautifulSoup(page.content, \"lxml\")\n",
    "            tables = soup.findAll('table')\n",
    "        \n",
    "                # build a dataframe of the win loss data\n",
    "            win_loss_df = pd.read_html(str(tables))[0]\n",
    "            win_loss_6_df = win_loss_df.head(12)\n",
    "            win_loss_6_df['team'] = team_name\n",
    "\n",
    "            wl_teams = wl_teams.append(win_loss_6_df , ignore_index=True)    \n",
    "\n",
    "        # clean the dataframe\n",
    "\n",
    "        wl_teams.columns = wl_teams.columns.droplevel(level=0)\n",
    "        wl_teams['team'] = wl_teams[\"\"]\n",
    "        wl_teams.drop(\"\"  , axis=1)\n",
    "        \n",
    "        self.data = wl_teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Websites of Interest\n",
    "\n",
    "\n",
    "we would like to compile a list of websites that we will scrape. We then will create a bball_scraper object, and feed it the list of urls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change team names to what is used in sports reference urls\n",
    "\n",
    "The team_id provides names written as 'Penn State'. The data we will be collecting is from Sports-reference.com. The url for penn state's data is 'https://www.sports-reference.com/cbb/schools/penn-state/2020.html' <br>\n",
    "1) the name needs to be adjusted to no spaces, all lower caps <br>\n",
    "2) the overall framework is 'https://www.sports-reference.com/cbb/schools/NAME/YEAR.html'<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "* this function will be used to transform a provided name to one that can be input into a sports-reference url\n",
    "* this will likely need to be adjusted as new teams are included\n",
    "* this needs a unit test (i.e. run all our team names through once to see if they pass)\n",
    "\"\"\"\n",
    "class name_cleaner():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.clean_names = []\n",
    "        self.num_errors = 0\n",
    "        \n",
    "    # not too happy about this\n",
    "    # names is a list of strings\n",
    "    def clean(self , names):\n",
    "        \n",
    "        clean_names = []\n",
    "        for name in names:\n",
    "            team = name\n",
    "            team = '!' + team + '!'                   # first and last character\n",
    "            team = team.replace(' ' , '-')            # no spaces\n",
    "            team = team.replace('(' , '')\n",
    "            team = team.replace(')' , '')\n",
    "            team = team.replace('.' , '')\n",
    "            team = team.replace( \"'\" , '')\n",
    "            team = team.replace(\"&\" , \"\")\n",
    "            team = team.lower()\n",
    "            team = team.replace('!southern-univ!' , '!southern!')\n",
    "            team = team.replace('!w-texas-am!' , '!west-texas-am!')\n",
    "            team = team.replace('!armstrong-st!' , '!armstrong!')\n",
    "            team = team.replace('ark-' , 'arkansas-')\n",
    "            team = team.replace('-st!' , '-state!')\n",
    "            team = team.replace('!n-' , '!north-')\n",
    "            team = team.replace('!e-' , '!eastern-')\n",
    "            team = team.replace('!cs-' , '!cal-state-')\n",
    "            team = team.replace('!s-illinois!' , '!southern-illinois!')\n",
    "            team = team.replace('!s-' , '!south-')\n",
    "            team = team.replace('!w-' , '!western-')\n",
    "            team = team.replace('!nc-' , '!north-carolina-')\n",
    "            team = team.replace('chr!' , 'christian!')\n",
    "            team = team.replace('-so!' , '-southern!')\n",
    "            team = team.replace('!fl-' , '!florida-')\n",
    "            team = team.replace('!uc-' , '!california-')\n",
    "            team = team.replace('intl!' , 'international!')\n",
    "            team = team.replace('cal-state-sacramento' , 'sacramento-state')\n",
    "            team = team.replace('univ!' , 'university!')\n",
    "            team = team.replace('miss!' , 'mississippi!')\n",
    "            team = team.replace('-slo' , '')\n",
    "            team = team.replace('car!' , 'carolina!')\n",
    "            team = team.replace('!ne-omaha!' , '!nebraska-omaha!')\n",
    "            team = team.replace('!ne-' , '!northeastern-')\n",
    "            team = team.replace('!ut-' , '!texas-')\n",
    "            team = team.replace('okla-' , 'oklahoma-')\n",
    "            team = team.replace('!penn!' , '!pennsylvania!')\n",
    "            team = team.replace('!unc' , '!north-carolina-')\n",
    "            team = team.replace('!american-university!' , '!american!')\n",
    "            team = team.replace('!detroit!' , '!detroit-mercy!')\n",
    "            team = team.replace('!loy-' , '!loyola-')\n",
    "            team = team.replace('!loyola-chicago!' , '!loyola-il!')\n",
    "            team = team.replace('!north-kentucky!' , '!northern-kentucky!')\n",
    "            team = team.replace('!north-illinois!' , '!northern-illinois!')\n",
    "            team = team.replace('!north-colorado!' , '!northern-colorado!')\n",
    "            team = team.replace('!cal-baptist!' , '!california-baptist!')\n",
    "            team = team.replace('!augusta!' , '!augusta-state!')\n",
    "            team = team.replace('!etsu!' , '!east-tennessee-state!')\n",
    "            team = team.replace('!mt-' , '!mount-')\n",
    "            team = team.replace('!g-washington!' , '!george-washington!')\n",
    "            team = team.replace('!ga-' , '!georgia-')\n",
    "            team = team.replace('!il-' , '!illinois-')\n",
    "            team = team.replace('!houston-bap!' , '!houston-baptist!')\n",
    "            team = team.replace('!kennesaw!' , '!kennesaw-state!')\n",
    "            team = team.replace('!bowling-green!' , '!bowling-green-state!')\n",
    "            team = team.replace('!col-charleston!' , '!college-of-charleston!')\n",
    "            team = team.replace('!cent-arkansas!' , '!central-arkansas!')\n",
    "            team = team.replace('!central-conn!' , '!central-connecticut-state!')\n",
    "            team = team.replace('!kent!' , '!kent-state!')\n",
    "            team = team.replace('!lsu!' , '!louisiana-state!')\n",
    "            team = team.replace('!ms-' , '!mississippi-')\n",
    "            team = team.replace('!f-dickinson!' , '!fairleigh-dickinson!')\n",
    "            team = team.replace('!byu!' , '!brigham-young!')\n",
    "            team = team.replace('!ma-' , '!massachusetts-')\n",
    "            team = team.replace('!northwestern-la!' , '!northwestern-state!')\n",
    "            team = team.replace('!long-island!' , '!long-island-university!')\n",
    "            team = team.replace('!wi-' , '!')\n",
    "            team = team.replace('!c-' , '!central-')\n",
    "            team = team.replace('!md-e-shore!' , '!maryland-eastern-shore!')\n",
    "            team = team.replace('!st-johns!' , '!st-johns-ny!')\n",
    "            team = team.replace('!tcu!' , '!texas-christian!')\n",
    "            team = team.replace('!tx-' , '!texas-')\n",
    "            team = team.replace('!va-' , '!virginia-')\n",
    "            team = team.replace('!vmi!' , '!virginia-millitary-institute!')\n",
    "            team = team.replace('!wku!' , '!western-kentucky!')\n",
    "            team = team.replace('!utep!' , '!texas-el-paso!')\n",
    "            team = team.replace('!st-marys-ca!' , '!saint-marys-ca!')\n",
    "            team = team.replace('!santa-barbara!' , '!california-santa-barbara!')\n",
    "            team = team.replace('!unlv!' , '!nevada-las-vegas!')\n",
    "            team = team.replace('-pa!' , '!')\n",
    "            team = team.replace('!uab!' , '!alabama-birmingham!')\n",
    "            team = team.replace('!mtsu!' , '!middle-tennessee!')\n",
    "            team = team.replace('!smu!' , '!southern-methodist!')\n",
    "            team = team.replace('!sf-austin!' , '!stephen-f-austin!')\n",
    "            team = team.replace('!st-josephs!' , '!saint-josephs!')\n",
    "            team = team.replace('!umbc!' , '!maryland-baltimore-county!')\n",
    "            team = team.replace('!usc!' , '!southern-california!')\n",
    "            team = team.replace('!st-peters!' , '!saint-peters!')\n",
    "            team = team.replace('!st-louis!' , '!saint-louis!')\n",
    "            team = team.replace('!ull!' , '!louisiana-lafayette!')\n",
    "            team = team.replace('!usc!' , '!southern-california!')\n",
    "            team = team.replace('--' , '-')\n",
    "            team = team.replace(\"university-of-\" , \"\")\n",
    "            team = team[1:-1]\n",
    "            \n",
    "            clean_names.append(team)\n",
    "        self.clean_names = clean_names\n",
    "\n",
    "\n",
    "    def test_names(self , other_names = None):\n",
    "        # we can use this to test the names it just cleaned, or an entirely different set of names\n",
    "        if other_names == None:\n",
    "            team_names = self.clean_names\n",
    "        else:\n",
    "            team_names = other_names\n",
    "        \n",
    "        \n",
    "        errors = 0\n",
    "        for name in team_names:\n",
    "            url_test =  \"https://www.sports-reference.com/cbb/schools/\" + name + \"/index.html\"\n",
    "            page_test = requests.get(url_test)   # scrape\n",
    "            soup_test = BeautifulSoup(page_test.content, \"lxml\")   # parse\n",
    "            tables_test = soup_test.findAll('table')    # search for tables \n",
    "            if len(tables_test) != 0:\n",
    "                print(\"                                 \" , name , \" Pass\")\n",
    "            else:\n",
    "                print(name , \" Fail\")\n",
    "                errors += 1\n",
    "        print(\"finished unit test. There were \" , errors , \" errors.\")\n",
    "        self.num_errors = errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate URLS\n",
    "\n",
    "once the names are put into the correct format, we can actually generate the urls. These urls will be saved into a list of dictionary's along with the team of interest's name, year and id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class url_generator():\n",
    "    \n",
    "    # team_array is a 2 by n array. where the first row is the year. the second row is the team_name\n",
    "    def __init__(self, ncaa_games , team_df):\n",
    "        self.teams = None\n",
    "        self.team_data = team_df\n",
    "        self.ncaa_games = ncaa_games\n",
    "        self.url_list_one = []\n",
    "        self.url_list_two = []\n",
    "\n",
    "\n",
    "    '''\n",
    "    team_data is a dataframe which has the columns: 'TeamID', 'TeamName', 'FirstD1Season', 'LastD1Season'.  \n",
    "    We want to creates a new column which has the team names in accordance to the sports-reference.com framework. \n",
    "    \n",
    "    First, reduce the team_data dataframe to only those teams who actually compteded in the tournament\n",
    "    '''\n",
    "    def select_teams(self):\n",
    "        s1 = set(self.team_data.TeamID)\n",
    "        s2 = set(self.ncaa_games.WTeamID.append(self.ncaa_games.LTeamID))\n",
    "        extra_teams = s1.symmetric_difference(s2)\n",
    "        for value in extra_teams:\n",
    "            self.team_data = self.team_data.drop(self.team_data[self.team_data[\"TeamID\"] == value].index)\n",
    "      \n",
    "        # get a list of all the team names in our dataframe\n",
    "        team_names = self.team_data.TeamName.values.tolist()\n",
    "        # clean every name in the team_data dataframe\n",
    "        mr_clean = name_cleaner()                          # make a name_cleaner object\n",
    "        mr_clean.clean(team_names)                         # clean the names\n",
    "        # test names and add cleaned names to df if no errors\n",
    "        #mr_clean.test_names()                              # test the names\n",
    "        \n",
    "        if mr_clean.num_errors ==0:                        # only go forward with no errors\n",
    "            self.team_data[\"SrNames\"] = mr_clean.clean_names\n",
    "        \n",
    "    '''\n",
    "    Goal: compile season long data for the teams who competed in the 2010 - 2018 NCAA tournaments. The idea is that the season long data is the information we will have in the future for creating predictions.\n",
    "\n",
    "        * in the ncaa_short, we have the year the game was played, and the id's of both teams\n",
    "        * in team_data we have the team id, along with the team name (cleaned for sports-reference use).\n",
    "    '''\n",
    "    \n",
    "    def build_array(self):\n",
    "       # get arrays which include the team years and id's \n",
    "        winner_array = np.vstack((self.ncaa_games.Season.values , self.ncaa_games.WTeamID.values ))\n",
    "        loser_array = np.vstack((self.ncaa_games.Season.values , self.ncaa_games.LTeamID.values))\n",
    "\n",
    "        # List of years\n",
    "        years = self.ncaa_games.Season.unique()\n",
    "\n",
    "        # the below loop will create a 2 x n array of all unique teams (year , school_id) which competed in NCAA's\n",
    "        all_teams = np.empty((2,0)) # fill this array\n",
    "        a_team_list = []\n",
    "        for year in years:\n",
    "            temp_1 = self.ncaa_games.WTeamID.loc[(self.ncaa_games.Season == year)].append(self.ncaa_games.LTeamID.loc[(self.ncaa_games.Season == year)]).unique()\n",
    "            a_team_list = a_team_list + temp_1.tolist()\n",
    "            temp_2 = np.full(shape = len(temp_1), fill_value = year , dtype = np.int)\n",
    "            temp_3 = np.vstack((temp_2 , temp_1)) \n",
    "            all_teams = np.hstack((all_teams , temp_3)) # fill array\n",
    "\n",
    "        # keep track of this\n",
    "        self.teams = all_teams\n",
    "        \n",
    "        \n",
    "    # there are two webpages per team on sports reference we would like to scrape\n",
    "    # for this webpage, we only want one row from the first table\n",
    "    def build_url_one(self):\n",
    "        url_one = []\n",
    "        for i in range(self.teams.shape[1]):   # this is the years \n",
    "            team = {}\n",
    "            team_name = self.team_data.SrNames.loc[(self.team_data.TeamID == self.teams[1 , i])].values.tolist()[0] # get the team Name\n",
    "            url = \"https://www.sports-reference.com/cbb/schools/\" + str(team_name) + \"/\" + str(int(self.teams[0 , i])) + \".html\" # season data\n",
    "           \n",
    "            # build the team dictionary\n",
    "            team['name'] = team_name\n",
    "            team['id']   = self.teams[1 , i]\n",
    "            team['year'] = self.teams[0 , i]\n",
    "            team['url']  = url \n",
    "            url_one.append(team)\n",
    "       \n",
    "        self.url_list_one = url_one\n",
    "        \n",
    "    # this webpage gives overview data for a team over a span of years.\n",
    "    # we only want a few of these years.\n",
    "    def build_url_two(self):\n",
    "        url_two = []\n",
    "        teamIDs = pd.unique(self.teams[1])    # we only want each team once\n",
    "        for j in range(len(teamIDs)): \n",
    "            team = {}\n",
    "            team_name = self.team_data.SrNames.loc[(self.team_data.TeamID == teamIDs[j])].values.tolist()[0]\n",
    "            url = \"https://www.sports-reference.com/cbb/schools/\" + team_name + \"/\"\n",
    "            \n",
    "            # build the team dictionary\n",
    "            team['name'] = team_name\n",
    "            team['id']   = teamIDs[j]\n",
    "            team['year'] = None\n",
    "            team['url']  = url \n",
    "            url_two.append(team)\n",
    "       \n",
    "        self.url_list_two = url_two\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because basketball has changed so much over the past years, we believe that data prior to the year 2009 could be detrimental to our model. For this reason, the next block of code will be used to slim the data down from the 1985 - 2018 seasons to a dataframe including the the 2020 - 2018 NCAA tournaments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a few constants\n",
    "start_year = 2009\n",
    "\n",
    "# read ncaa tournament results into dataframe ( data obtained from Kaggle)\n",
    "ncaa_total = pd.read_csv('NCAATourneyCompactResults.csv')  \n",
    "# read team ID dataframe (from Kaggle)\n",
    "team_data = pd.read_csv('teams.csv')  \n",
    "# get the first index for the desired year\n",
    "start = min(ncaa_total[ncaa_total.iloc[:,0] == start_year].index.values.astype(int)) \n",
    "# shorten our dataframe\n",
    "ncaa_short = ncaa_total.iloc[start: , :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture [--no-stderr] \n",
    "\n",
    "# create data pipeline object\n",
    "pipe = collect_pipeline()\n",
    "pipe.make_folder()\n",
    "\n",
    "# make the spider object\n",
    "spider = bball_scraper()\n",
    "\n",
    "# make url_generator object\n",
    "go_daddy = url_generator( ncaa_short , team_data)\n",
    "\n",
    "# Step 1: Make the urls \n",
    "go_daddy.select_teams()\n",
    "go_daddy.build_array()\n",
    "go_daddy.build_url_one()\n",
    "go_daddy.build_url_two()\n",
    "\n",
    "# parse the first\n",
    "spider.set_urls(go_daddy.url_list_one)\n",
    "spider.parse_one()\n",
    "pipe.write_data_one(spider)\n",
    "\n",
    "# parse the second\n",
    "spider.set_urls(go_daddy.url_list_two)\n",
    "spider.parse_two()\n",
    "pipe.write_data_two(spider)\n",
    "\n",
    "# save the other\n",
    "pipe.write_team_list(go_daddy.team_data)\n",
    "pipe.write_game_list(go_daddy.ncaa_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file finished running\n"
     ]
    }
   ],
   "source": [
    "print(\"file finished running\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
