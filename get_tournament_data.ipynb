{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect This Year's Tournament Data\n",
    "This should only be ran one time, and on the correct day - when all regular season games are finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our imports for the model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import lxml.html as lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class collect_pipeline(object):\n",
    "    \n",
    "    def make_folder(self):\n",
    "        if not os.path.exists('./tournament_data'):\n",
    "            os.mkdir('./tournament_data')\n",
    "    def join_data(self, spider):\n",
    "        season_stats = pd.merge(spider.data_one , spider.data_two , on=[ 'Team'  ] , how = 'left')\n",
    "        cols_keep = ['Team' , 'FG' , 'FGA', 'FG%', '2P' , '2PA', '2P%', '3P' , '3PA', '3P%', 'FT' , 'FTA', 'FT%', 'ORB',\n",
    "       'DRB', 'TRB' , 'AST', 'STL', 'BLK', 'TOV', 'PF',  'PTS/G','W-L%', 'SRS', 'SOS','PTS.1','Seed' ]\n",
    "        season_stats = season_stats[cols_keep]\n",
    "        season_stats = season_stats.rename(columns = {\"PTS.1\": \"opp_PPG\"})\n",
    "        self.joined_data = season_stats\n",
    "        \n",
    "    def write_data(self):\n",
    "        self.joined_data.to_csv('tournament_data/prelim_data.csv' , index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class bball_scraper():\n",
    "   \n",
    "\n",
    "    def __init__(self ):\n",
    "        self.start_urls = []\n",
    "        self.data_one = None\n",
    "        self.data_two = None\n",
    "    \n",
    "    # url_list is a list of dictionary's. each has a 'name' id' , 'year' , 'url'\n",
    "    # for the second group of urls - data will be none type\n",
    "    def set_urls(self , url_list):\n",
    "        self.start_urls = url_list\n",
    "    \n",
    "    # this is the first set of urls. only need the first url from one table\n",
    "    def parse_one(self):\n",
    "        season_stats = pd.DataFrame()   # start with an empty dataframe\n",
    "        print('here')\n",
    "        for item in self.start_urls:\n",
    "            # extact the information we need\n",
    "            team_name = item['name']\n",
    "            url = item['url']\n",
    "            year = item['year']\n",
    "            page = requests.get(url)\n",
    "            soup = BeautifulSoup(page.content, \"lxml\")\n",
    "            tables = soup.findAll('table')    # search for tables \n",
    "            # convert to a dataframe and label the data\n",
    "            df = pd.read_html(str(tables[1]))[0]      # select the table of interest into a pandas dataframe  \n",
    "            df.drop([1 , 2, 3] , inplace=True)\n",
    "            df = df.replace('Team' , team_name)   # want the name of the team \n",
    "            df['Date'] = year  # only want the year in this column\n",
    "            df = df.rename(columns={'Unnamed: 0': 'Team'})\n",
    "\n",
    "            \n",
    "            season_stats = season_stats.append(df , ignore_index = True , sort=False)\n",
    "           # season_stats = pd.concat([season_stats, df] ,  axis=0) # add this to the season stats empty dataframe we started with\n",
    "              # simply clean up the columns \n",
    "           \n",
    "            # save the data\n",
    "        self.data_one = season_stats  \n",
    "            \n",
    "        # this is the second set of urls - more than one row from the table\n",
    "    def parse_two(self):\n",
    "        wl_teams = pd.DataFrame()      # initialize and empty dataframe\n",
    "        for item in self.start_urls:\n",
    "                # extract the info\n",
    "            team_name = item['name']\n",
    "            url = item['url']\n",
    "                # parse the page\n",
    "            page = requests.get(url)\n",
    "            soup = BeautifulSoup(page.content, \"lxml\")\n",
    "            tables = soup.findAll('table')\n",
    "        \n",
    "                # build a dataframe of the win loss data\n",
    "            win_loss_df = pd.read_html(str(tables))[0]\n",
    "            win_loss_6_df = win_loss_df.head(1)\n",
    "            win_loss_6_df['Team' , 'Team'] = team_name\n",
    "            win_loss_6_df['Unnamed: 17_level_0', 'Seed'] = item[\"seed\"] \n",
    "            wl_teams = wl_teams.append(win_loss_6_df , ignore_index=True)    \n",
    "\n",
    "        # clean the dataframe\n",
    "        wl_teams.columns = wl_teams.columns.droplevel(level=0)\n",
    "        wl_teams.columns =  ['Rk', 'Season', 'Conf', 'W', 'L', 'W-L%', 'W.1', 'L.1', 'W-L%.1', 'SRS', 'SOS', 'PTS', 'PTS.1', 'AP Pre', 'AP High', 'AP Final', 'NCAA Tournament', 'Seed', 'Coach(es)', 'Team']\n",
    "\n",
    "\n",
    "     #   wl_teams['team'] = wl_teams[\"\"]\n",
    "        #wl_teams.drop(\"\"  , axis=1)\n",
    "        \n",
    "        self.data_two = wl_teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate URLS\n",
    "\n",
    "once the names are put into the correct format, we can actually generate the urls. These urls will be saved into a list of dictionary's along with the team of interest's name, year and id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do:\n",
    "    # fill out this list\n",
    "    # decide what framework the teams should be in. \n",
    "\n",
    "ncaa_teams = {\n",
    "    'south' : [\n",
    "        'kansas' , \n",
    "        'san-diego-state' , \n",
    "        'maryland' , \n",
    "        'kentucky' , \n",
    "        'butler' , \n",
    "        'auburn' , \n",
    "        'west-virginia' , \n",
    "        'saint-marys-ca' , \n",
    "        'oklahoma' , \n",
    "        'texas-tech' , \n",
    "        'arizona-state' , \n",
    "        'purdue' , \n",
    "        'richmond' , \n",
    "        'tennessee' , \n",
    "        'rhode-island' , \n",
    "        'mississippi-state' \n",
    "        ] , \n",
    "        'east' : [\n",
    "        'gonzaga' , \n",
    "        'florida-state' , \n",
    "        'louisville' , \n",
    "        'villanova' , \n",
    "        'penn-state' , \n",
    "        'iowa' , \n",
    "        'arizona' , \n",
    "        'houston' , \n",
    "        'louisiana-state' , \n",
    "        'xavier' , \n",
    "        'texas' , \n",
    "        'tulsa' , \n",
    "        'vermont' , \n",
    "        'belmont' , \n",
    "        'wright-state' , \n",
    "        'siena' \n",
    "        ]  ,\n",
    "        'midwest' : [\n",
    "        'baylor' , \n",
    "        'seton-hall' , \n",
    "        'duke' , \n",
    "        'oregon' , \n",
    "        'brigham-young' ,  # byu\n",
    "        'colorado' , \n",
    "        'illinois' , \n",
    "        'indiana' , \n",
    "        'providence' , \n",
    "        'florida' , \n",
    "        'northern-iowa' , \n",
    "        'liberty' , \n",
    "        'akron' , \n",
    "        'yale' , \n",
    "        'radford' , \n",
    "        'hofstra' \n",
    "        ]  ,\n",
    "        'west' : [\n",
    "        'dayton' , \n",
    "        'michigan-state' , \n",
    "        'creighton' , \n",
    "        'ohio-state' , \n",
    "        'wisconsin' , \n",
    "        'michigan' , \n",
    "        'virginia' , \n",
    "        'marquette' , \n",
    "        'southern-california' , \n",
    "        'rutgers' , \n",
    "        'east-tennessee-state' , \n",
    "        'stephen-f-austin' , \n",
    "        'cincinnati' , \n",
    "        'wichita-state' , \n",
    "        'utah-state' , \n",
    "        'north-carolina-central' \n",
    "        ]\n",
    "    \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "class url_generator():\n",
    "    # teams is a dictionary of names. defined by region, a list of sports -reference\n",
    "    # url based names of teams. the teams are in seed order.\n",
    "    def __init__(self, team_names):\n",
    "        self.teams = []\n",
    "        self.team_names = team_names\n",
    "        self.url_list_one = []\n",
    "        self.url_list_two = []\n",
    "        self.year = 2020\n",
    "\n",
    "    def build_array(self):\n",
    "        # for this, we want to build a 3 row array -> name , year , seed. (we don't necessarily know \n",
    "        # if seed will be readily availible in time)\n",
    "       \n",
    "        for region in self.team_names:\n",
    "            team_list = self.team_names[region]\n",
    "            for i in range(len(team_list)):\n",
    "                seed = i+1\n",
    "                sr_name = team_list[i]\n",
    "                year = self.year\n",
    "                self.teams += [[sr_name , year , seed]]\n",
    "                \n",
    "                 \n",
    "    # there are two webpages per team on sports reference we would like to scrape\n",
    "    # for this webpage, we only want one row from the first table\n",
    "    def build_url_one(self):\n",
    "        url_one = []\n",
    "        for i in range(len(self.teams)):   # this is over every team\n",
    "            team = {}\n",
    "            team_name = self.teams[i][0]\n",
    "            year = self.year\n",
    "\n",
    "            url = \"https://www.sports-reference.com/cbb/schools/\" + str(team_name) + \"/\" + str(year) + \".html\" # season data\n",
    "            # build the team dictionary\n",
    "            team['name'] = team_name\n",
    "            team['year'] = self.year\n",
    "            team['url']  = url \n",
    "            url_one.append(team)\n",
    "       \n",
    "        self.url_list_one = url_one\n",
    "        \n",
    "    # this webpage gives overview data for a team over a span of years.\n",
    "    # we only want a few of these years.\n",
    "    def build_url_two(self):\n",
    "        url_two = []\n",
    "        for i in range(len(self.teams)):\n",
    "            team = {}\n",
    "            team_name = self.teams[i][0]\n",
    "            seed = self.teams[i][2]\n",
    "            url = \"https://www.sports-reference.com/cbb/schools/\" + team_name + \"/\"\n",
    "            \n",
    "            # build the team dictionary\n",
    "            team['name'] = team_name\n",
    "            team['year'] = None\n",
    "            team['url']  = url \n",
    "            team['seed'] = seed\n",
    "            url_two.append(team)\n",
    "       \n",
    "        self.url_list_two = url_two\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture [--no-stderr] \n",
    "print(\"start\")\n",
    "# create data pipeline object\n",
    "pipe = collect_pipeline()\n",
    "pipe.make_folder()\n",
    "\n",
    "# make the spider object\n",
    "spider = bball_scraper()\n",
    "\n",
    "# make url_generator object\n",
    "go_daddy = url_generator( ncaa_teams )\n",
    "\n",
    "# Step 1: Make the urls \n",
    "go_daddy.build_array()\n",
    "go_daddy.build_url_one()\n",
    "go_daddy.build_url_two()\n",
    "\n",
    "# parse the first\n",
    "spider.set_urls(go_daddy.url_list_one)\n",
    "spider.parse_one()\n",
    "\n",
    "\n",
    "# parse the second\n",
    "spider.set_urls(go_daddy.url_list_two)\n",
    "spider.parse_two()\n",
    "\n",
    "\n",
    "\n",
    "# save the other\n",
    "pipe = collect_pipeline()\n",
    "pipe.join_data(spider)\n",
    "pipe.write_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished_running\n"
     ]
    }
   ],
   "source": [
    "print(\"finished_running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
