{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect This Year's Tournament Data\n",
    "This should only be ran one time, and on the correct day - when all regular season games are finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our imports for the model\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import lxml.html as lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class collect_pipeline(object):\n",
    "\n",
    "    def make_folder(self):\n",
    "        if not os.path.exists('./tournament_data'):\n",
    "            os.mkdir('./tournament_data')\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class bball_scraper():\n",
    "   \n",
    "\n",
    "    def __init__(self ):\n",
    "        self.start_urls = []\n",
    "        self.data_one = None\n",
    "    \n",
    "    # url_list is a list of dictionary's. each has a 'name' id' , 'year' , 'url'\n",
    "    # for the second group of urls - data will be none type\n",
    "    def set_urls(self , url_list):\n",
    "        self.start_urls = url_list\n",
    "    \n",
    "    # this is the first set of urls. only need the first url from one table\n",
    "    def parse_one(self):\n",
    "        season_stats = pd.DataFrame()   # start with an empty dataframe\n",
    "        \n",
    "        for item in self.start_urls:\n",
    "            # extact the information we need\n",
    "            team_id = item['id']\n",
    "            team_name = item['name']\n",
    "            url = item['url']\n",
    "            year = item['year']\n",
    "            page = requests.get(url)\n",
    "            soup = BeautifulSoup(page.content, \"lxml\")\n",
    "            tables = soup.findAll('table')    # search for tables \n",
    "            # convert to a dataframe and label the data\n",
    "            df = pd.read_html(str(tables[1]))[0]      # select the table of interest into a pandas dataframe  \n",
    "            df.drop([1 , 2, 3] , inplace=True)\n",
    "            df = df.replace('Team' , team_name)   # want the name of the team \n",
    "            df['Team_ID'] = team_id        # put in the team ID's\n",
    "            df['Date'] = year  # only want the year in this column\n",
    "            df = df.rename(columns={'Unnamed: 0': 'Team'})\n",
    "\n",
    "            \n",
    "            season_stats = season_stats.append(df , ignore_index = True , sort=False)\n",
    "           # season_stats = pd.concat([season_stats, df] ,  axis=0) # add this to the season stats empty dataframe we started with\n",
    "              # simply clean up the columns \n",
    "            \n",
    "            # save the data\n",
    "            self.data = season_stats  \n",
    "            \n",
    "        # this is the second set of urls - more than one row from the table\n",
    "    def parse_two(self):\n",
    "        wl_teams = pd.DataFrame()      # initialize and empty dataframe\n",
    "        for item in self.start_urls:\n",
    "                # extract the info\n",
    "            team_idea = item['id']\n",
    "            team_name = item['name']\n",
    "            url = item['url']\n",
    "                   \n",
    "                # parse the page\n",
    "            page = requests.get(url)\n",
    "            soup = BeautifulSoup(page.content, \"lxml\")\n",
    "            tables = soup.findAll('table')\n",
    "        \n",
    "                # build a dataframe of the win loss data\n",
    "            win_loss_df = pd.read_html(str(tables))[0]\n",
    "            win_loss_6_df = win_loss_df.head(12)\n",
    "            win_loss_6_df['team'] = team_name\n",
    "\n",
    "            wl_teams = wl_teams.append(win_loss_6_df , ignore_index=True)    \n",
    "\n",
    "        # clean the dataframe\n",
    "\n",
    "        wl_teams.columns = wl_teams.columns.droplevel(level=0)\n",
    "        wl_teams['team'] = wl_teams[\"\"]\n",
    "        wl_teams.drop(\"\"  , axis=1)\n",
    "        \n",
    "        self.data = wl_teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
